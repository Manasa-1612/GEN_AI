{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e9b8e3",
   "metadata": {},
   "source": [
    "# Foundations of NLP-From Tokenization to Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d0eda",
   "metadata": {},
   "source": [
    "# 1.Tokenization\n",
    "Tokenize a paragraph into sentences and words using nltk.sent_tokenize() and nltk.word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4728f413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word_Tokens: ['Hi', ',', 'I', 'am', 'Manasa', '.', 'I', 'am', 'the', 'Trainee', 'of', 'Skill', 'Share', 'Technologies', '.', 'I', 'am', 'learning', 'Machine', 'Learning', 'from', 'the', 'faculty', 'of', 'Skill', 'Share', '.']\n",
      "Sentence_Tokens: ['Hi , I am Manasa .', 'I am the Trainee of Skill Share Technologies .', 'I am learning Machine Learning from the faculty of Skill Share.']\n"
     ]
    }
   ],
   "source": [
    "#Step-1: Import required Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "\n",
    "# Step-2: Download the necessary NLTK Resources\n",
    "nltk.download(\"punkt\") \n",
    "\n",
    "'''\n",
    "# step-3: Sample Sentence\n",
    "input_text = \" Environment pollution is a complex and urgent global issue that threatens the health of our planet and its \" \\\n",
    "\"inhabitants . It encompasses air pollution , water pollution, and soil pollution , each with its set of consequences . Air\" \\\n",
    "\" pollution , primarily caused by industrial activities and transportation, results in respiratory illnesses , cardiovascular\" \\\n",
    "\" problems , and environmental damage . Water pollution , often due to industrial discharges and inadequate waste management , \" \\\n",
    "\"endangers aquatic ecosystems and human health . Soil pollution, stemming from the use of chemicals and improper waste disposal , \" \\\n",
    "\" harms agriculture and food security . Awareness and education play a crucial role in addressing pollution . By adopting eco-friendly \" \\\n",
    "\"lifestyles , conserving resources, and supporting policies that prioritize environmental protection , we can contribute to a cleaner , \" \\\n",
    "\" healthier planet . Pollution knows no borders , and it affects us all . It is our responsibility to take action and work together to\" \\\n",
    "\" ensure a sustainable and pollution-free future for generations to come . Environmental pollution is the contamination of air , water , \" \\\n",
    "\"and land by harmful substances . The increasing number of factories , vehicles , and the disposal of waste in water bodies are major\" \\\n",
    "\" causes of pollution . It harms not only the environment but also human health . The burning of fossil fuels and deforestation further\" \\\n",
    "\" contribute to air pollution and global warming . To reduce pollution , we need to use eco-friendly products , plant trees , and reduce\" \\\n",
    "\" waste . We must all take responsibility for protecting our environment for a better tomorrow.\"\n",
    "'''\n",
    "\n",
    "# Step-3: Create a Paragraph\n",
    "input_text = input(\"Enter input_text\")\n",
    "\n",
    "# step-4: Perform Text-Tokenization\n",
    "\n",
    "# Word_Tokenization\n",
    "word_token = word_tokenize(input_text)\n",
    "\n",
    "# Sentence_Tokenization\n",
    "sentence_token = sent_tokenize(input_text)\n",
    "\n",
    "# Step-5: Display the result\n",
    "print(\"Word_Tokens:\",word_token)\n",
    "print(\"Sentence_Tokens:\",sentence_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0abca8",
   "metadata": {},
   "source": [
    "What is Tokenization?\n",
    "Tokenization is the process of breaking down a text like a sentence or paragraph into smaller units called tokens. These tokens can be words,sentence depending on the type of tokenization used.\n",
    "\n",
    "Why Tokenization is important in Natural Language Processing?\n",
    "Machines can't process raw text like humans.In such cases we use Tokenization, which breaks the complete text into smaller, meaningful units (tokens) that computers can work with. So that the  each token can be analyzed and produces the output again in human Understandable Language.\n",
    "\n",
    "Code Explanation\n",
    "----------------\n",
    "import nltk: Here We are importing the Natural Language Toolkit (NLTK), which is a powerful Python library for NLP,used for processing human language data.\n",
    "We are using this nltk library to access functions like tokenizers (word_tokenize, sent_tokenize), stopwords, POS tagging, etc.\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize: We are specifically importing two functions:\n",
    "sent_tokenize: For splitting text into sentences.\n",
    "\n",
    "word_tokenize: For splitting text into words and punctuation.\n",
    "We are using the above functions because, Tokenization is the first step in text analysis; these functions help break down the text for further processing.\n",
    "\n",
    "nltk.download(\"punkt\"):We are downloading a pre-trained tokenizer model called \"punkt\".\n",
    "The punkt model is needed by sent_tokenize and word_tokenize to understand sentence boundaries and word patterns.Without this line, those functions would raise an error.\n",
    "\n",
    "input_text = input(\"Enter input_text\"): We are asking the user to enter a string (a paragraph or sentence), which is saved in the variable input_text.\n",
    "We need some text to tokenize — this lets the user provide that text during runtime.\n",
    "\n",
    "word_token = word_tokenize(input_text):This function splits the input text into a list of words and punctuation marks.\n",
    "\n",
    "sentence_token = sent_tokenize(input_text):This function splits the input text into a list of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc9795",
   "metadata": {},
   "source": [
    "# 2.Text Preprocessing-Stemming\n",
    "    Use PorterStemmer to reduce words to their base form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0d16276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input_Text: Hi , I am Manasa . I am the Trainee of Skill Share Technologies . I am learning Machine Learning from the faculty of Skill Share.\n",
      "Stemmed_Text: ['hi', ',', 'i', 'am', 'manasa', '.', 'i', 'am', 'the', 'traine', 'of', 'skill', 'share', 'technolog', '.', 'i', 'am', 'learn', 'machin', 'learn', 'from', 'the', 'faculti', 'of', 'skill', 'share', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step-1: Import required Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "'''\n",
    "#Step-2: Download the necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "'''\n",
    "# Step-3: Initialize the Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Step-4: Sample Sentence\n",
    "input_text1 = \"Hi , I am Manasa . I am the Trainee of Skill Share Technologies . I am learning Machine Learning from the faculty of Skill Share.\"\n",
    "'''\n",
    "# Step-4: Create a Paragraph\n",
    "input_text1 = input(\"Enter input_text:\")\n",
    "'''\n",
    "# Step-5: Tokenize the paragraph(Split into words)\n",
    "word_token = word_tokenize(input_text1)\n",
    "\n",
    "# Step-6: Apply Stemming\n",
    "stemmed_text = [stemmer.stem(text) for text in word_token]\n",
    "\n",
    "# Step-7: Display Result\n",
    "print(\"Original Input_Text:\",input_text1)\n",
    "print(\"Stemmed_Text:\",stemmed_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaddee0",
   "metadata": {},
   "source": [
    "What is Stemming?\n",
    "Stemming is the process of reducing a word to its base or stem form — often by chopping off suffixes like -ing, -ed, -ly, etc.\n",
    "Here we are using the PorterStemmer, which is a rule-based stemmer that removes common endings.\n",
    "\n",
    "Difference between Stem and Root?\n",
    "Stem:The part of a word that remains after removing suffixes or prefixes (may not be a valid word)\n",
    "     Output\tMay be incomplete or not meaningful in English\n",
    "     Example: “Running” Stem: \"run\" (Correct in this case)\t\n",
    "     Example: “Studies”\tStem: \"studi\" (Incorrect English word)\n",
    "     Speed:Fast (simple rules)\n",
    "Root:The original, meaningful core word from which related words are derived\n",
    "\t output Usually a valid word in the language\n",
    "     Example: “Running” Root: \"run\"\n",
    "     Example: “Studies”\tRoot: \"study\"\n",
    "     Speed:Slower (requires deeper analysis)\n",
    "\n",
    "Why stemming may affect the meaning?\n",
    "Stemming can affect the meaning of words because that removes affixes (like -ing, -ed, -es, etc.) without understanding the context or grammar of the word.\n",
    "Stemming algorithms produce stems that are not valid English words.These stems don’t carry clear meaning on their own and may not be understood by humans.\n",
    "Examples:\n",
    "\"connect\", \"connection\", \"connected\" → \"connect\": (this is fine)\n",
    "\"universe\", \"university\" → \"univers\" :(very different meanings, same stem)\n",
    "\n",
    "Code Explanation\n",
    "----------------\n",
    "import nltk: Imports the Natural Language Toolkit library — used for processing human language data.\n",
    "\n",
    "from nltk.stem import PorterStemmer: Imports the PorterStemmer class — a popular rule-based stemming algorithm that removes common endings like -ing, -ed, -s, etc.\n",
    "\n",
    "stemmer = PorterStemmer():Here we created an object called stemmer for the PorterStemmer class.\n",
    "We need to use this object(stemmer) to call the .stem() method on each word you want to reduce to its stem.\n",
    "\n",
    "stemmed_text = [stemmer.stem(text) for text in word_token]:Uses a list comprehension to stem each word token.\n",
    "stem(text) takes each word and reduces it to its root-like form.\n",
    "\n",
    "Example:\"learning\" → \"learn\",\"Technologies\" → \"technolog\"\n",
    "We use Stemming because it helps to reduce the different forms of a word to a common base, making text analysis simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f10027",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1a75c9a",
   "metadata": {},
   "source": [
    "# 3. Text Processing-Lemmatization\n",
    "     Use WordNetLemmatizer and compare with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03cb8e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_text: Hi , I am Manasa . I am the Trainee of Skill Share Technologies . I am learning Machine Learning from the faculty of Skill Share.\n",
      "Lemmatized_word: ['Hi', ',', 'I', 'am', 'Manasa', '.', 'I', 'am', 'the', 'Trainee', 'of', 'Skill', 'Share', 'Technologies', '.', 'I', 'am', 'learning', 'Machine', 'Learning', 'from', 'the', 'faculty', 'of', 'Skill', 'Share', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step-1: Import required Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Step-2: Download necessary NLTK resources\n",
    "#nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Step-3: Initialize the Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step-4: Create a Paragraph\n",
    "input_text2 = \"Hi , I am Manasa . I am the Trainee of Skill Share Technologies . I am learning Machine Learning from the faculty of Skill Share.\"\n",
    "'''\n",
    "# Step-4: Create a Paragraph\n",
    "input_text2 = input(\"Enter Paragraph:\")\n",
    "'''\n",
    "\n",
    "# Step-5: Tokenize the Sentence\n",
    "word_token2 = word_tokenize(input_text2)\n",
    "\n",
    "# Step-6: Applying Lemmatization\n",
    "lemmatize_word = [lemmatizer.lemmatize(text) for text in word_token2]\n",
    "\n",
    "# Step-7: Display the result\n",
    "print(\"Original_text:\",input_text2)\n",
    "print(\"Lemmatized_word:\",lemmatize_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16358161",
   "metadata": {},
   "source": [
    "What is Lemmatization?\n",
    "Lemmatization means changing a word to its basic or dictionary form.\n",
    "It understands the meaning and grammar of the word, so it gives you the correct root word.It helps computers understand words better by reducing different word forms to one standard word, so it's easier to analyze and compare text.\n",
    "\n",
    "\n",
    "When is Lemmatization more appropriate than Stemming?\n",
    "Lemmatization returns proper words found in the dictionary.\n",
    "Stemming may return broken or meaningless roots.\n",
    "Example:\n",
    "Stemming: \"studies\" → \"studi\" \n",
    "Lemmatization: \"studies\" → \"study\" \n",
    "\n",
    "Code Explanation\n",
    "----------------\n",
    "WordNetLemmatizer: A tool from NLTK to convert words to their dictionary form (lemma).We use WordNetLemmatizer to prepare text for further processing like searching, classification, or language understanding.\n",
    "\n",
    "\"wordnet\": A large dictionary database of English words (WordNet), used for finding lemmas.\n",
    "\"omw-1.4\": Open Multilingual WordNet, needed for word meanings and translations (improves accuracy).\n",
    "\"punkt\" : Required for tokenizing text into words or sentences \n",
    "We download above NLP resources because Lemmatization needs a vocabulary to work correctly. These downloads provide that knowledge.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer(): We are creating an object of the WordNetLemmatizer class so we can use it later.\n",
    "We need this object to call the function .lemmatize() on each word.\n",
    "\n",
    "lemmatize_word = [lemmatizer.lemmatize(text) for text in word_token2]:This is a list comprehension. It runs .lemmatize() on each word in your tokenized list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce73731",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ea7de19",
   "metadata": {},
   "source": [
    "# 4. Stopwords Removal\n",
    "Remove common stopwords using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d56ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Text: Hi , I am Manasa . I am the Trainee of Skill Share Technologies . I am learning Machine Learning from the faculty of Skill Share.\n",
      "Filtered_text: ['Hi', ',', 'Manasa', '.', 'Trainee', 'Skill', 'Share', 'Technologies', '.', 'learning', 'Machine', 'Learning', 'faculty', 'Skill', 'Share', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step-1: Import the Libraries\n",
    "# import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step-2: Download necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Step-3: Generate a Sentence or a paragraph by Static or Dynamic \n",
    "# STATIC\n",
    "input_text4 = \"Hi , I am Manasa . I am the Trainee of Skill Share Technologies . I am learning Machine Learning from the faculty of Skill Share.\"\n",
    "# DYNAMIC\n",
    "# input_text4 = input(\"Enter INPUT:\")\n",
    "\n",
    "# Step-4: Perform Tokenization(word_tokenization)\n",
    "words = word_tokenize(input_text4)\n",
    "\n",
    "# Step-5: Stopwords in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step-6: Remove Stopwords\n",
    "filtered_words =[word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "# Step-7: Display Result\n",
    "print(\"Original_Text:\",input_text4)\n",
    "print(\"Filtered_text:\",filtered_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7f236",
   "metadata": {},
   "source": [
    "What are Stopwords?\n",
    "Stopwords are the common words in a language that are often filtered out (removed) during text processing because they usually carry less important meaning in terms of NLP tasks.\n",
    "These words appear frequently, but they usually don’t help much in understanding the main meaning or intent of a sentence.\n",
    "\n",
    "When should we keep or remove Stopwords?\n",
    "We remove the stopwords when they\n",
    "Do not add meaningful information\n",
    "Increase data size without benefit\n",
    "Slow down algorithms unnecessarily\n",
    "\n",
    "Code Explanation:\n",
    "-----------------\n",
    "stopwords: A module from NLTK containing lists of common words (e.g., \"is\", \"the\", \"and\") in different languages.We need these tools to split the text and filter out the common words.\n",
    "nltk.download(\"stopwords\"): Downloads lists of stopwords in multiple languages (you’ll use English).These are required for stopword removal to work properly.\n",
    "stop_words = set(stopwords.words('english')): Loads a list of common English words like \"the\", \"is\", \"am\"...... \n",
    "These words usually don’t help in meaning and can be filtered out for many NLP tasks.\n",
    "\n",
    "filtered_words =[word for word in words if word.lower() not in stop_words]: It's a list comprehension.\n",
    "Converts each word to lowercase (to match stopword list) and includes it only if it’s not a stopword.\n",
    "This step cleans the data by removing noise. It improves the focus on important words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af63afe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "476912db",
   "metadata": {},
   "source": [
    "# 5. Parts of Speech(POS) Tagging\n",
    "  Use nltk.pos_tag() on a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c068e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_Text: Hi , I am Manasa . I am the Trainee of Skill Share Technologies . I am learning Machine Learning from the faculty of Skill Share.\n",
      "input_text4 and Tags: [('Hi', 'NNP'), (',', ','), ('I', 'PRP'), ('am', 'VBP'), ('Manasa', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('the', 'DT'), ('Trainee', 'NNP'), ('of', 'IN'), ('Skill', 'NNP'), ('Share', 'NNP'), ('Technologies', 'NNPS'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Machine', 'NNP'), ('Learning', 'NNP'), ('from', 'IN'), ('the', 'DT'), ('faculty', 'NN'), ('of', 'IN'), ('Skill', 'NNP'), ('Share', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Step1: Import Libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step-2: Download the necessary NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "\n",
    "# Step-3: Generate a Sentence or a paragraph by Static or Dynamic \n",
    "# STATIC\n",
    "input_text4 = \"Hi , I am Manasa . I am the Trainee of Skill Share Technologies . I am learning Machine Learning from the faculty of Skill Share.\"\n",
    "# DYNAMIC\n",
    "# input_text4 = input(\"Enter INPUT:\")\n",
    "\n",
    "# Step-4: Perform Tokenization(word_tokenization)\n",
    "word_token4 = word_tokenize(input_text4)\n",
    "\n",
    "# Step-5: Applying POS tags\n",
    "tags = nltk.pos_tag(word_token4)\n",
    "\n",
    "# Step-6: Display Result\n",
    "print(\"Original_Text:\",input_text4)\n",
    "print(\"input_text4 and Tags:\",tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b039065",
   "metadata": {},
   "source": [
    "What is POS Tagging?\n",
    "POS tagging is the process of identifying the part of speech (like noun, verb, adjective, etc.) for each word in a sentence.\n",
    "It tells us about,what role each word plays in the sentence.\n",
    "\n",
    "Importance of POS Tagging in Syntantic and Semantic Analysis?\n",
    "Syntactic analysis focuses on the structure of a sentence. POS tagging helps by:\n",
    "\n",
    "How POS Tagging Helps\n",
    " Sentence Parsing:\tIdentifies how words are structured (subject-verb-object). Example: “Manasa reads books” → NNP VBZ NNS\n",
    " Phrase Chunking :\tHelps in forming noun phrases, verb phrases.\n",
    " Disambiguation\t :  Helps resolve grammar ambiguities (e.g., “book” as a noun vs. verb).\n",
    " Grammar Checking:\tDetects syntactic errors by checking POS sequences.\n",
    "\n",
    " Semantic analysis focuses on the meaning behind words and phrases. POS tagging contributes by:\n",
    "\n",
    "How POS Tagging Helps\n",
    "Word Sense Disambiguation:\tKnowing a word’s POS helps pick its correct meaning.\n",
    "Sentiment Analysis       :\tIdentifies key adjectives, adverbs (e.g., happy, badly).\n",
    "Entity Recognition       :\tHelps tag proper nouns (people, places, etc.).\n",
    "Question Answering       :\tHelps determine expected answer types based on question words (e.g., “Where” → location, “Who” → person).\n",
    "\n",
    "Code Explanation\n",
    "----------------\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\"): Downloads the POS tagger model that uses the \"Averaged Perceptron\" algorithm to assign parts of speech.\n",
    "These datasets and models are required dependencies. Without them, the tokenizer and tagger won't work.\n",
    "\n",
    "nltk.pos_tag(word_token4): Assigns each word in word_token4 a POS tag (like noun, verb, adjective).\n",
    "This is the core of the program. It tells you what grammatical role each word plays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eaebf8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6583cca",
   "metadata": {},
   "source": [
    "# 6. Named Entity Recognition(NER)\n",
    "Perform NER using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6802e225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "Hi\n",
      ",\n",
      "I\n",
      "am\n",
      "Manasa\n",
      ".\n",
      "I\n",
      "am\n",
      "the\n",
      "Trainee\n",
      "of\n",
      "Skill\n",
      "Share\n",
      "Technologies\n",
      ".\n",
      "I\n",
      "am\n",
      "learning\n",
      "Machine\n",
      "Learning\n",
      "from\n",
      "the\n",
      "faculty\n",
      "of\n",
      "Skill\n",
      "Share\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Step-1: Import Libraries\n",
    "import spacy\n",
    "\n",
    "# Step-2: Load\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Hi , I am Manasa . I am the Trainee of Skill Share Technologies . I am learning Machine Learning from the faculty of Skill Share.\"\n",
    "doc = nlp(text)\n",
    "print(\"Tokens:\")\n",
    "for ent in doc:\n",
    "    print(ent.text)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e42381",
   "metadata": {},
   "source": [
    "What is NER?\n",
    "NER stands for Named Entity Recognition.\n",
    "It is a Natural Language Processing (NLP) technique used to Identify and classify named entities in a text into predefined categories such as:\n",
    "Person names,Locations,Organizations,Dates,Times,Monetary values,Percentages.\n",
    "\n",
    "How NER is used in real-world applications like resumes,news,etc?\n",
    "1.Resumes\n",
    "Automatically extract and organize key information from job applicants' resumes.\n",
    "Uses:\n",
    "Speeds up resume screening\n",
    "Enables applicant ranking\n",
    "Fills candidate profiles automatically\n",
    "\n",
    "2. News Classification & Summarization\n",
    "Identify key people, places, and events in news articles.\n",
    "Uses:\n",
    "Summarize news into key points\n",
    "Group articles by topic or people\n",
    "Detect breaking news or trends\n",
    "\n",
    "Code Explanation\n",
    "----------------\n",
    "import spacy: This line imports the spaCy library. spaCy is used for advanced NLP tasks like tokenization, POS tagging, Named Entity Recognition (NER), and more.\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy.load(\"en_core_web_sm\") loads a pre-trained English model.\n",
    "\"en_core_web_sm\" stands for:\n",
    "\"en\" = English language\n",
    "\"core\" = core (basic) features\n",
    "\"web\" = trained on web data\n",
    "\"sm\" = small (lightweight version)\n",
    "nlp is now a pipeline object that processes English text (tokenizes, tags, recognizes entities, etc.)\n",
    "doc = nlp(text):This line passes the input text through the spaCy pipeline using the nlp model.\n",
    "doc is a spaCy Doc object, which contains:Tokenized words,POS tags,Named entities\n",
    "for ent in doc:\n",
    "    print(ent.text)\n",
    "We are looping through the doc object.\n",
    "ent represents each token in the processed text .\n",
    "ent.text returns the original word or punctuation symbol.\n",
    "So, this loop prints all the tokens one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51e657",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "827d9a2e",
   "metadata": {},
   "source": [
    "# 7. One Hot Encoding\n",
    "Use OneHotEncoder to encode Categorical variables like gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf88f8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender_Female  Gender_Male  Gender_Other\n",
      "0            0.0          1.0           0.0\n",
      "1            1.0          0.0           0.0\n",
      "2            0.0          1.0           0.0\n",
      "3            1.0          0.0           0.0\n",
      "4            0.0          0.0           1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Gender':['Male','Female','Male','Female','Other']\n",
    "})\n",
    "'''\n",
    "df = pd.read_csv(\"C:/python_Files/housing.csv\")\n",
    "x = df.iloc[:,-1]\n",
    "x_reshaped = x.values.reshape(-1, 1)\n",
    "'''\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded = encoder.fit_transform(df[['Gender']])\n",
    "encoded_df = pd.DataFrame(encoded,columns=encoder.get_feature_names_out(['Gender']))\n",
    "print(encoded_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fad159",
   "metadata": {},
   "source": [
    "How OneHotEncoding works?\n",
    "One-Hot Encoding is a technique to convert text data (like words) into numerical format so that machines can understand and process them.\n",
    "We need OneHotEncoding in NLP because Computers can’t understand text directly, but they can work with numbers. So, we convert words into numbers using techniques like:\n",
    "One-Hot Encoding,Bag of Words,TF-IDF,Word Embeddings (Word2Vec, GloVe)\n",
    "One-Hot Encoding is the simplest among them.\n",
    "\n",
    "Real-life uses(e.g:customer segmentation)\n",
    "Customer segmentation is the process of dividing customers into groups based on shared characteristics, such as:\n",
    "Age,Gender,Location,Purchase behavior,Product preference\n",
    "Businesses use this to personalize marketing, improve recommendations, and target promotions effectively.\n",
    "\n",
    "What happens with unknown labels?\n",
    "When using One-Hot Encoding, unknown labels (i.e., labels not seen during training) can cause issues—especially if we are transforming new data that includes categories not present during fit().\n",
    "Example:Gender: ['Male', 'Female']\n",
    "After fitting the encoder, we try to transform:\n",
    "Gender: ['Other']  # <-- new, unseen label\n",
    "This is an unknown category, and by default, it will cause an error.\n",
    "\n",
    "Code Explanation\n",
    "----------------\n",
    "import pandas as pd\n",
    "pandas is a library used for handling structured data.\n",
    "pd is just an alias used for convenience,Needed to read the CSV file and handle data in tabular format (DataFrames).\n",
    "from sklearn.preprocessing import OneHotEncoder:Imports the OneHotEncoder class from scikit-learn.\n",
    "encoder = OneHotEncoder(sparse_output=False):Creates an instance of the OneHotEncoder.\n",
    "sparse_output=False: outputs a dense NumPy array instead of a sparse matrix (easier to print and work with).\n",
    " We want a human-readable table instead of a compressed sparse format.\n",
    "encoded = encoder.fit_transform(x_reshaped)\n",
    "Fits the encoder to the data (fit) and transforms it (transform) in one step.\n",
    "The result encoded is a 2D NumPy array with 1s and 0s, where each column represents a category.\n",
    "This is the actual One-Hot Encoding step that converts categorical text into numeric format.\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['Gender]))\n",
    "Converts the NumPy encoded array back into a pandas DataFrame for easy viewing and further processing,becauseMakes the encoded data readable and usable like the original DataFrame.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
