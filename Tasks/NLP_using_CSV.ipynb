{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b703c2",
   "metadata": {},
   "source": [
    "## JAGATHA MANASA                                                                                                             1/06/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e2f44",
   "metadata": {},
   "source": [
    "# Performing NLP tasks by loading CSV File\n",
    "## 1. Load the dataset and perform Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3117eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I love programming in Python.\n",
      "Word_Tokenization: ['I', 'love', 'programming', 'in', 'Python', '.']\n",
      "  \n",
      "Original Sentence: Natural Language Processing is fascinating.\n",
      "Word_Tokenization: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '.']\n",
      "  \n",
      "Original Sentence: Spacy and NLTK are popular NLP libraries.\n",
      "Word_Tokenization: ['Spacy', 'and', 'NLTK', 'are', 'popular', 'NLP', 'libraries', '.']\n",
      "  \n",
      "Original Sentence: Machine learning enables predictive analysis.\n",
      "Word_Tokenization: ['Machine', 'learning', 'enables', 'predictive', 'analysis', '.']\n",
      "  \n",
      "Original Sentence: Data preprocessing is a crucial step in NLP.\n",
      "Word_Tokenization: ['Data', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'NLP', '.']\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Step-1: Import libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step-2: Download necessary resources\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Step-3: Load the dataset using pandas\n",
    "data = pd.read_csv(\"NLP_Assignment_Sentences.csv\")\n",
    "\n",
    "# Step-4: Extract the necessary columns from the entire dataset(Slicing)\n",
    "x = data['Sentence']\n",
    "\n",
    "# Step-5: Process the sentences \n",
    "for row in x:\n",
    "\n",
    "    # Step-6: Perform word_tokenization\n",
    "    tokens = word_tokenize(row)\n",
    "\n",
    "    # Step-7: Display the Result\n",
    "    print(\"Original Sentence:\",row)\n",
    "    print(\"Word_Tokenization:\",tokens)\n",
    "    print(\"  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092942b3",
   "metadata": {},
   "source": [
    "\n",
    "## Code Objective:\n",
    "\n",
    "We are using NLTK to tokenize words in sentences from a CSV file. \n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    ".nltk:nltk is a natural language processing library.\n",
    ".pandas:pandas is used to read and handle the CSV file.\n",
    ".word_tokenize():word_tokenize() is a function from nltk.tokenize that splits a sentence into individual words and punctuation tokens.\n",
    ".data = pd.read_csv():Loads our CSV file into a DataFrame called data.\n",
    ".x = data['Sentence']:Extracts just the \"Sentence\" column from our DataFrame.x is now a Pandas Series containing all sentences.\n",
    ".for loop:Iterates over each sentence (row) in the Sentence column.Uses word_tokenize() to split the every sentence into words and punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e5955",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing Using PorterStemmer and WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a515eec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I love programming in Python.\n",
      "Tokenized_words: ['I', 'love', 'programming', 'in', 'Python', '.']\n",
      "Stemmed_text: ['i', 'love', 'program', 'in', 'python', '.']\n",
      "Lemmatized_text: ['I', 'love', 'programming', 'in', 'Python', '.']\n",
      "   \n",
      "Original Sentence: Natural Language Processing is fascinating.\n",
      "Tokenized_words: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '.']\n",
      "Stemmed_text: ['natur', 'languag', 'process', 'is', 'fascin', '.']\n",
      "Lemmatized_text: ['Natural', 'Language', 'Processing', 'is', 'fascinating', '.']\n",
      "   \n",
      "Original Sentence: Spacy and NLTK are popular NLP libraries.\n",
      "Tokenized_words: ['Spacy', 'and', 'NLTK', 'are', 'popular', 'NLP', 'libraries', '.']\n",
      "Stemmed_text: ['spaci', 'and', 'nltk', 'are', 'popular', 'nlp', 'librari', '.']\n",
      "Lemmatized_text: ['Spacy', 'and', 'NLTK', 'are', 'popular', 'NLP', 'library', '.']\n",
      "   \n",
      "Original Sentence: Machine learning enables predictive analysis.\n",
      "Tokenized_words: ['Machine', 'learning', 'enables', 'predictive', 'analysis', '.']\n",
      "Stemmed_text: ['machin', 'learn', 'enabl', 'predict', 'analysi', '.']\n",
      "Lemmatized_text: ['Machine', 'learning', 'enables', 'predictive', 'analysis', '.']\n",
      "   \n",
      "Original Sentence: Data preprocessing is a crucial step in NLP.\n",
      "Tokenized_words: ['Data', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'NLP', '.']\n",
      "Stemmed_text: ['data', 'preprocess', 'is', 'a', 'crucial', 'step', 'in', 'nlp', '.']\n",
      "Lemmatized_text: ['Data', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'NLP', '.']\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "# Step-1: Import necessary functions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer , WordNetLemmatizer\n",
    "\n",
    "# Step-2: Initialize Stemmer and lemmatizer\n",
    "Stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Step-3: Extract the necessary columns from the entire dataset(Slicing)\n",
    "x = data['Sentence']\n",
    "\n",
    "# Step-4: Process the sentences \n",
    "for row in x:\n",
    "\n",
    "    # Step-5: Perform word_tokenization\n",
    "    word_tokens = word_tokenize(row)\n",
    "    \n",
    "    # Step-6: Apply Stemming and Lemmatization\n",
    "    stemmed_text = [Stemmer.stem(text) for text in word_tokens]\n",
    "    lemmatized_text = [lemmatizer.lemmatize(text) for text in word_tokens]\n",
    "\n",
    "    # Step-7: Display the Result\n",
    "    print(\"Original Sentence:\",row)\n",
    "    print(\"Tokenized_words:\",word_tokens)\n",
    "    print(\"Stemmed_text:\",stemmed_text)\n",
    "    print(\"Lemmatized_text:\",lemmatized_text)\n",
    "    print(\"   \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68faaeae",
   "metadata": {},
   "source": [
    "\n",
    "## Code Objective:\n",
    "\n",
    "It processes each sentence from a CSV file, and for each sentence:\n",
    ".Tokenizes the sentence into words.\n",
    ".Stems each word using the PorterStemmer.\n",
    ".Lemmatizes each word using the WordNetLemmatizer.\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    ".word_tokenize: Splits sentences into words.\n",
    ".PorterStemmer: Reduces words to their root form.\n",
    ".WordNetLemmatizer: Reduces words to their base dictionary form.\n",
    ".Stemmer = PorterStemmer()\n",
    ".lemmatizer = WordNetLemmatizer()\n",
    "  These create instances of the stemmer and lemmatizer.\n",
    ".for loop:This loop goes through each sentence in the dataset.Uses word_tokenize() to split the every sentence into words and punctuation.\n",
    ".stemmed_text = [Stemmer.stem(text) for text in word_tokens]:Applies stemming to each word.\n",
    ".lemmatized_text = [lemmatizer.lemmatize(text) for text in word_tokens]:Applies lemmatization to each word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0104d",
   "metadata": {},
   "source": [
    "## 3.StopWords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f971598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I love programming in Python.\n",
      "Filtered_Text: ['love', 'programming', 'Python', '.']\n",
      " \n",
      "Original Sentence: Natural Language Processing is fascinating.\n",
      "Filtered_Text: ['Natural', 'Language', 'Processing', 'fascinating', '.']\n",
      " \n",
      "Original Sentence: Spacy and NLTK are popular NLP libraries.\n",
      "Filtered_Text: ['Spacy', 'NLTK', 'popular', 'NLP', 'libraries', '.']\n",
      " \n",
      "Original Sentence: Machine learning enables predictive analysis.\n",
      "Filtered_Text: ['Machine', 'learning', 'enables', 'predictive', 'analysis', '.']\n",
      " \n",
      "Original Sentence: Data preprocessing is a crucial step in NLP.\n",
      "Filtered_Text: ['Data', 'preprocessing', 'crucial', 'step', 'NLP', '.']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Step-1: Import necessary functions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step-2: Download necessary resources\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Step-3: Extract the necessary columns from the entire dataset(Slicing)\n",
    "x = data['Sentence']\n",
    "\n",
    "# Step-4: Process the sentences \n",
    "for row in x:\n",
    "\n",
    "    # Step-5: Perform word_tokenization\n",
    "    word_token = word_tokenize(row)\n",
    "\n",
    "    # Step-6: Stopwords in English\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Step-7: Remove Stopwords\n",
    "    filtered_words = [word for word in word_token if word.lower() not in stop_words]\n",
    "\n",
    "    # Step-8: Display the Result\n",
    "    print(\"Original Sentence:\",row)\n",
    "    print(\"Filtered_Text:\",filtered_words)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608f925b",
   "metadata": {},
   "source": [
    "\n",
    "## Code Objective:\n",
    "\n",
    "Tokenize each sentence in our dataset.\n",
    ".Remove common stopwords like \"is\", \"the\", \"in\", etc., which are not useful for many NLP tasks.\n",
    ".Print the filtered (cleaned) version of each sentence.\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    ".stopwords: A built-in NLTK list of very common words in English (like \"the\", \"is\", \"at\", etc.) which adds little meaning in tasks like classification.\n",
    ".word_tokenize: Splits a sentence into individual tokens (words and punctuation).\n",
    ".for loop:This loop goes through each sentence in the dataset.Uses word_tokenize() to split the every sentence into words and punctuation.\n",
    "filtered_words = [word for word in word_token if word.lower() not in stop_words]:\n",
    "     .Loops through each token word\n",
    "     .Converts it to lowercase with word.lower() to handle case insensitivity\n",
    "     .Keeps it only if it is NOT in the stop words set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a49debc",
   "metadata": {},
   "source": [
    "## 4.Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c5fd2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I love programming in Python.\n",
      "Text Tokens and Tags: [('I', 'PRP'), ('love', 'VBP'), ('programming', 'VBG'), ('in', 'IN'), ('Python', 'NNP'), ('.', '.')]\n",
      " \n",
      "Original Sentence: Natural Language Processing is fascinating.\n",
      "Text Tokens and Tags: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('fascinating', 'VBG'), ('.', '.')]\n",
      " \n",
      "Original Sentence: Spacy and NLTK are popular NLP libraries.\n",
      "Text Tokens and Tags: [('Spacy', 'NN'), ('and', 'CC'), ('NLTK', 'NNP'), ('are', 'VBP'), ('popular', 'JJ'), ('NLP', 'NNP'), ('libraries', 'NNS'), ('.', '.')]\n",
      " \n",
      "Original Sentence: Machine learning enables predictive analysis.\n",
      "Text Tokens and Tags: [('Machine', 'NN'), ('learning', 'VBG'), ('enables', 'NNS'), ('predictive', 'JJ'), ('analysis', 'NN'), ('.', '.')]\n",
      " \n",
      "Original Sentence: Data preprocessing is a crucial step in NLP.\n",
      "Text Tokens and Tags: [('Data', 'NNP'), ('preprocessing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('crucial', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('NLP', 'NNP'), ('.', '.')]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Step-1: Import necessary functions\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Step-2: Download necessary resources\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Step-3: Extract the necessary columns from the entire dataset(Slicing)\n",
    "x = data['Sentence']\n",
    "\n",
    "# Step-4: Process the sentences \n",
    "for row in x:\n",
    "\n",
    "    # Step-5: Perform word_tokenization\n",
    "    token = word_tokenize(row)\n",
    "\n",
    "    # Step-6: Applying pos_tag\n",
    "    tags = nltk.pos_tag(token)\n",
    "\n",
    "    # Step-7: Display the Result\n",
    "    print(\"Original Sentence:\",row)\n",
    "    print(\"Text Tokens and Tags:\",tags)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f2ca2",
   "metadata": {},
   "source": [
    "\n",
    "## Code Objective:\n",
    "\n",
    ".Tokenizes each sentence from our dataset.\n",
    ".Applies POS tagging to each token (e.g., identifying nouns, verbs, adjectives).\n",
    ".Prints the tagged result for each sentence.\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    ".nltk.download('averaged_perceptron_tagger'):Downloads the pretrained POS tagger model required to label each word with its part of speech.\n",
    ".for loop:This loop goes through each sentence in the dataset.Uses word_tokenize() to split the every sentence into words and punctuation.\n",
    ".tags = nltk.pos_tag(token):Applies POS tagging to each token using NLTK’s pos_tag() function.\n",
    ".It returns a list of tuples, where each tuple is:('word', 'POS-tag')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bfd815",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition(NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d94b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                    I love programming in Python.\n",
      "1      Natural Language Processing is fascinating.\n",
      "2        Spacy and NLTK are popular NLP libraries.\n",
      "3    Machine learning enables predictive analysis.\n",
      "4     Data preprocessing is a crucial step in NLP.\n",
      "Name: Sentence, dtype: object\n",
      "  Named Entities:\n",
      "Python -> GPE\n",
      " \n",
      "  Named Entities:\n",
      "Natural Language Processing -> ORG\n",
      " \n",
      "  Named Entities:\n",
      "Spacy -> PERSON\n",
      " \n",
      "NLTK -> ORG\n",
      " \n",
      "NLP -> ORG\n",
      " \n",
      "Sentence does not contain any Named Entity\n",
      "  Named Entities:\n",
      "NLP -> ORG\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# Step-1: Import libraries\n",
    "import spacy\n",
    "\n",
    "# Step-2: Load the English Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Step-3: Extract the necessary columns from the entire dataset(Slicing)\n",
    "x = data['Sentence']\n",
    "print(x)\n",
    "# Step-4: Process the sentences \n",
    "for row in x:\n",
    "    doc = nlp(row)\n",
    "\n",
    "    # Step-5: Display the Result\n",
    "    if doc.ents:\n",
    "        print(\"  Named Entities:\")\n",
    "        for ent in doc.ents:\n",
    "            print(f\"{ent.text} -> {ent.label_}\")\n",
    "            print(\" \")\n",
    "    else:\n",
    "        print(\"Sentence does not contain any Named Entity\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8553a65b",
   "metadata": {},
   "source": [
    "\n",
    "## Code Objective:\n",
    "\n",
    "This code uses spaCy's built-in NER capabilities to extract and print named entities from each sentence in a CSV file column.\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    ".import spacy:This imports the spaCy NLP library, which is powerful for tasks like tokenization, POS tagging, and especially Named Entity Recognition (NER).\n",
    ".spacy.load(\"en_core_web_sm\"):This line loads the English small model (en_core_web_sm).\n",
    "This model is pre-trained and includes:Tokenizer,POS tagger,Named Entity Recognizer.\n",
    ".nlp(row):Passes the sentence (row) through the NLP pipeline.\n",
    "doc is a spaCy object that contains all the linguistic information about the sentence:Tokens,POS tags,Named Entities (via doc.ents)\n",
    ".doc.ents: returns a list of named entities (like person names, organizations, dates, etc.).If the list is not empty, the sentence contains named entities.\n",
    ".{ent.text} -> {ent.label_}:For each entity:\n",
    "    .ent.text: The actual named entity in the sentence (e.g., \"Python\").\n",
    "    .ent.label_: The type of the entity (e.g., ORG for organization, GPE for location)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7c2a3",
   "metadata": {},
   "source": [
    "# 6. One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb03c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Word  Word_and  Word_are  Word_fascinating.  Word_i  Word_in  \\\n",
      "0              i       0.0       0.0                0.0     1.0      0.0   \n",
      "1           love       0.0       0.0                0.0     0.0      0.0   \n",
      "2    programming       0.0       0.0                0.0     0.0      0.0   \n",
      "3             in       0.0       0.0                0.0     0.0      1.0   \n",
      "4        python.       0.0       0.0                0.0     0.0      0.0   \n",
      "5        natural       0.0       0.0                0.0     0.0      0.0   \n",
      "6       language       0.0       0.0                0.0     0.0      0.0   \n",
      "7     processing       0.0       0.0                0.0     0.0      0.0   \n",
      "8             is       0.0       0.0                0.0     0.0      0.0   \n",
      "9   fascinating.       0.0       0.0                1.0     0.0      0.0   \n",
      "10         spacy       0.0       0.0                0.0     0.0      0.0   \n",
      "11           and       1.0       0.0                0.0     0.0      0.0   \n",
      "12          nltk       0.0       0.0                0.0     0.0      0.0   \n",
      "13           are       0.0       1.0                0.0     0.0      0.0   \n",
      "14       popular       0.0       0.0                0.0     0.0      0.0   \n",
      "15           nlp       0.0       0.0                0.0     0.0      0.0   \n",
      "16    libraries.       0.0       0.0                0.0     0.0      0.0   \n",
      "\n",
      "    Word_is  Word_language  Word_libraries.  Word_love  Word_natural  \\\n",
      "0       0.0            0.0              0.0        0.0           0.0   \n",
      "1       0.0            0.0              0.0        1.0           0.0   \n",
      "2       0.0            0.0              0.0        0.0           0.0   \n",
      "3       0.0            0.0              0.0        0.0           0.0   \n",
      "4       0.0            0.0              0.0        0.0           0.0   \n",
      "5       0.0            0.0              0.0        0.0           1.0   \n",
      "6       0.0            1.0              0.0        0.0           0.0   \n",
      "7       0.0            0.0              0.0        0.0           0.0   \n",
      "8       1.0            0.0              0.0        0.0           0.0   \n",
      "9       0.0            0.0              0.0        0.0           0.0   \n",
      "10      0.0            0.0              0.0        0.0           0.0   \n",
      "11      0.0            0.0              0.0        0.0           0.0   \n",
      "12      0.0            0.0              0.0        0.0           0.0   \n",
      "13      0.0            0.0              0.0        0.0           0.0   \n",
      "14      0.0            0.0              0.0        0.0           0.0   \n",
      "15      0.0            0.0              0.0        0.0           0.0   \n",
      "16      0.0            0.0              1.0        0.0           0.0   \n",
      "\n",
      "    Word_nlp  Word_nltk  Word_popular  Word_processing  Word_programming  \\\n",
      "0        0.0        0.0           0.0              0.0               0.0   \n",
      "1        0.0        0.0           0.0              0.0               0.0   \n",
      "2        0.0        0.0           0.0              0.0               1.0   \n",
      "3        0.0        0.0           0.0              0.0               0.0   \n",
      "4        0.0        0.0           0.0              0.0               0.0   \n",
      "5        0.0        0.0           0.0              0.0               0.0   \n",
      "6        0.0        0.0           0.0              0.0               0.0   \n",
      "7        0.0        0.0           0.0              1.0               0.0   \n",
      "8        0.0        0.0           0.0              0.0               0.0   \n",
      "9        0.0        0.0           0.0              0.0               0.0   \n",
      "10       0.0        0.0           0.0              0.0               0.0   \n",
      "11       0.0        0.0           0.0              0.0               0.0   \n",
      "12       0.0        1.0           0.0              0.0               0.0   \n",
      "13       0.0        0.0           0.0              0.0               0.0   \n",
      "14       0.0        0.0           1.0              0.0               0.0   \n",
      "15       1.0        0.0           0.0              0.0               0.0   \n",
      "16       0.0        0.0           0.0              0.0               0.0   \n",
      "\n",
      "    Word_python.  Word_spacy  \n",
      "0            0.0         0.0  \n",
      "1            0.0         0.0  \n",
      "2            0.0         0.0  \n",
      "3            0.0         0.0  \n",
      "4            1.0         0.0  \n",
      "5            0.0         0.0  \n",
      "6            0.0         0.0  \n",
      "7            0.0         0.0  \n",
      "8            0.0         0.0  \n",
      "9            0.0         0.0  \n",
      "10           0.0         1.0  \n",
      "11           0.0         0.0  \n",
      "12           0.0         0.0  \n",
      "13           0.0         0.0  \n",
      "14           0.0         0.0  \n",
      "15           0.0         0.0  \n",
      "16           0.0         0.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Step 1: Get first 3 sentences\n",
    "x = data['Sentence'].iloc[:3]\n",
    "\n",
    "# Step 2: Tokenize sentences into words and flatten\n",
    "tokenized = [sentence.lower().split() for sentence in x]\n",
    "flat_words = [[word] for sentence in tokenized for word in sentence]  # Must be 2D for encoder\n",
    "\n",
    "\n",
    "# Step 3: Initialize and apply OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded = encoder.fit_transform(flat_words)\n",
    "\n",
    "# Step 4: Create DataFrame with proper column names\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['Word']))\n",
    "\n",
    "\n",
    "# Optional: Add original words as a reference\n",
    "words = [word[0] for word in flat_words]\n",
    "encoded_df.insert(0, 'Word', words)\n",
    "\n",
    "# Print result\n",
    "print(encoded_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f18efc",
   "metadata": {},
   "source": [
    "\n",
    "## Code Objective:\n",
    "\n",
    ".Takes the first 3 sentences from a dataset.\n",
    ".Tokenize them into individual words.\n",
    ".Apply One-Hot Encoding using sklearn's OneHotEncoder.\n",
    ".Return a DataFrame where each word is represented as a one-hot encoded vector.\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder:Imports the OneHotEncoder from scikit-learn, which transforms categorical text data into binary vectors.\n",
    ".data['Sentence'].iloc[:3]:Selects the first 3 sentences from that column.\n",
    ".[sentence.lower().split() for sentence in x]:Converts each sentence to lowercase and splits it into words.\n",
    ".[[word] for sentence in tokenized for word in sentence]:Flattens the list of lists and wraps each word in its own list.This is required because OneHotEncoder expects a 2D array, where each row is a sample (in this case, a word).\n",
    "OneHotEncoder(sparse_output=False):Creates a OneHotEncoder object.\n",
    ".encoder.fit_transform(flat_words):Fits the encoder to the word list and transforms the words into binary vectors.\n",
    "Each word becomes a vector with 1 in the position of that word’s column and 0 elsewhere.\n",
    ".pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['Word'])):Creates a pandas DataFrame from the encoded result.\n",
    ".get_feature_names_out(['Word']) returns the column names, e.g., ['Word_love', 'Word_python.', ...].\n",
    ".words = [word[0] for word in flat_words]:Extracts the original words from the nested list flat_words.\n",
    ".encoded_df.insert(0, 'Word', words):Inserts the original words as the first column of the DataFrame for easier interpretation.\n",
    ".print(encoded_df):Displays the final one-hot encoded table, where each row represents a word, and the columns are binary indicators showing whether that word matches a specific vocabulary word.\n",
    "\n",
    "### Why OneHotEncoding is useful?\n",
    "\n",
    "Machine learning algorithms need numeric input. One-hot encoding is a common way to represent text data.\n",
    "You can use this encoded data as input features to models like logistic regression, SVM, etc.\n",
    "While this is simple and interpretable, for larger vocabularies or deep learning tasks, embeddings like Word2Vec or BERT are more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a80e13",
   "metadata": {},
   "source": [
    "### Display the output when unknown word is recognized using OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ede7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded value for 'transformer': [35]\n"
     ]
    }
   ],
   "source": [
    "# Step-1: Import libraries\n",
    "import tensorflow \n",
    "\n",
    "# Step-2: import necessary functions\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "\n",
    "# Step-3: Define a vocabulary size (required!) for hashing\n",
    "vocab_size = 100\n",
    "# Step-4: Initializing Unknown Word\n",
    "word = \"transformer\"\n",
    "\n",
    "# Step-5: Perform OneHotEncoding for Unknown Word\n",
    "encoded = one_hot(word, vocab_size)\n",
    "\n",
    "# Step-6: Display the Result\n",
    "print(f\"One-hot encoded value for '{word}':\", encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cc24d6",
   "metadata": {},
   "source": [
    "\n",
    "## Code Objective:\n",
    "\n",
    ".We are using a hashing-based encoding — it doesn't care whether the word was seen before. Instead, it uses a hash function to turn any word into a fixed integer index.\n",
    ".Even if a word is unknown, it still gets encoded to an integer. This allows the model to:\n",
    ".Handle new test-time words\n",
    ".Avoid crashing or failing due to “unknown token” errors\n",
    "\n",
    "### Code Explanation:\n",
    "\n",
    ".from tensorflow.keras.preprocessing.text import one_hot:This imports the one_hot function from Keras.\n",
    "📌 Note: one_hot here is not true one-hot encoding — it’s a hashing-based index generator.\n",
    ".word = \"transformer\":We are choosing a word that’s not in our training data to simulate how unknown words are handled.\n",
    ".one_hot(word, vocab_size):This line hashes the word and gives you an integer index — a pseudo-ID of that word.\n",
    "     .For \"transformer\" and vocab_size = 100, it might return something like [35].\n",
    "     .Even if \"transformer\" was never seen in training, it still gets a consistent ID (35).\n",
    "     .It doesn’t look at the actual vocabulary list.\n",
    "     .It maps any word to an integer in the range 1 to vocab_size.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
