{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b245b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_text</th>\n",
       "      <th>generated_headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Science: Score each cause. Quality throughout ...</td>\n",
       "      <td>NASA Discovers New Exoplanet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Environment: Behavior benefit suggest page. Ro...</td>\n",
       "      <td>Climate Change Effects Escalate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sports: Director allow firm environment. Tree ...</td>\n",
       "      <td>Local Team Wins Championship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports: Seven medical blood personal success m...</td>\n",
       "      <td>Local Team Wins Championship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Environment: Yet practice just military buildi...</td>\n",
       "      <td>Climate Change Effects Escalate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        content_text  \\\n",
       "0  Science: Score each cause. Quality throughout ...   \n",
       "1  Environment: Behavior benefit suggest page. Ro...   \n",
       "2  Sports: Director allow firm environment. Tree ...   \n",
       "3  Sports: Seven medical blood personal success m...   \n",
       "4  Environment: Yet practice just military buildi...   \n",
       "\n",
       "                generated_headline  \n",
       "0     NASA Discovers New Exoplanet  \n",
       "1  Climate Change Effects Escalate  \n",
       "2     Local Team Wins Championship  \n",
       "3     Local Team Wins Championship  \n",
       "4  Climate Change Effects Escalate  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CSV data into a pandas DataFrame\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"news_headline_generator.csv\")\n",
    "\n",
    "# Display the first few rows to inspect the structure of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b2bff",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> import pandas : importing the library pandas.\n",
    "-> data = pd.read_csv(\"news_headline_generator.csv\") : Load CSV data into a pandas DataFrame.\n",
    "-> data.head() : Display the first few rows to inspect the structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89a7bdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content_text          0\n",
       "generated_headline    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39a70c0",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> data.isnull().sum() : Check for missing values in the dataset.\n",
    "  -> if data.isnull().sum() = 0 then no NAN values are present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "132a7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'content_text' as the input features\n",
    "x = data['content_text']\n",
    "\n",
    "# Extract 'generated_headline' as the target labels\n",
    "y = data['generated_headline']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b52b2e",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> x = data['content_text'] : Extract 'content_text' as the input features\n",
    "\n",
    "-> y = data['generated_headline'] : Extract 'generated_headline' as the target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29bc9646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Science: Score each cause. Quality throughout ...\n",
      "1      Environment: Behavior benefit suggest page. Ro...\n",
      "2      Sports: Director allow firm environment. Tree ...\n",
      "3      Sports: Seven medical blood personal success m...\n",
      "4      Environment: Yet practice just military buildi...\n",
      "                             ...                        \n",
      "995    Health: Particularly state visit mention heart...\n",
      "996    Science: Floor feeling her play new win. Prove...\n",
      "997    Technology: Window foreign forward society enj...\n",
      "998    Environment: Position fact democratic vote rat...\n",
      "999    Environment: Board skin expect door magazine l...\n",
      "Name: content_text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93dc66ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             NASA Discovers New Exoplanet\n",
      "1          Climate Change Effects Escalate\n",
      "2             Local Team Wins Championship\n",
      "3             Local Team Wins Championship\n",
      "4          Climate Change Effects Escalate\n",
      "                      ...                 \n",
      "995    New Breakthrough in Cancer Research\n",
      "996           NASA Discovers New Exoplanet\n",
      "997           AI Revolutionizes Daily Life\n",
      "998        Climate Change Effects Escalate\n",
      "999        Climate Change Effects Escalate\n",
      "Name: generated_headline, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8c255914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# Clean text function to remove unwanted characters and normalize\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "\n",
    "        # Remove all characters except alphabets and space\n",
    "        text = re.sub('[^a-zA-Z ]', ' ', text)\n",
    "\n",
    "        # Remove extra spaces\n",
    "        text = ' '.join(text.split())\n",
    "\n",
    "        # Convert to lowercase\n",
    "        return text.lower()\n",
    "    return \"\"\n",
    "\n",
    "# Apply cleaning to both input and output texts\n",
    "data['content_text'] = data['content_text'].apply(clean_text)\n",
    "data['generated_headline'] = data['generated_headline'].apply(clean_text)\n",
    "\n",
    "# Add <start> and <end> tokens to target sequences\n",
    "# Add start and end tokens to help decoder learn boundaries\n",
    "# This is essential for sequence-to-sequence modeling\n",
    "data['generated_headline'] = data['generated_headline'].apply(lambda x: 'start ' + x + ' end')\n",
    "\n",
    "# Separate cleaned inputs and outputs\n",
    "x = data['content_text']\n",
    "y = data['generated_headline']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7f0fd",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "                                   TEXT CLEANING + FORMATTING FOR SEQ2SEQ\n",
    "-> import re\n",
    "   -> Imports Python’s built-in Regular Expression (regex) module.\n",
    "   -> Reason : Required for pattern-based text substitution and cleaning (e.g., removing non-alphabet characters).\n",
    "   -> Purpose: Helps in cleaning the raw text by removing special characters, numbers, and extra whitespace\n",
    "\n",
    "-> def clean_text(text):\n",
    "   -> Defines a custom function named clean_text that takes a single argument text.\n",
    "   -> Reason : Modularizes the text cleaning process, so it can be reused on multiple text fields.\n",
    "   -> Purpose: To ensure all input and output text is cleaned in a consistent way before feeding it to the model.\n",
    "\n",
    "-> if isinstance(text, str):\n",
    "   -> Checks if text is a string.\n",
    "   -> Reason : Prevents errors if text is NaN or another non-string data type.\n",
    "   -> Purpose: Defensive programming — ensures cleaning is applied only on valid strings.\n",
    "\n",
    "-> text = re.sub('[^a-zA-Z ]', ' ', text)\n",
    "   -> Replaces everything except alphabets and spaces with a space.\n",
    "   -> Reason : Removes numbers, punctuation, special characters (e.g., .,?!@).\n",
    "   -> Purpose: Keeps the text simple and clean — only words. Models like LSTM/GRU perform better with cleaner data.\n",
    "\n",
    "-> text = ' '.join(text.split())\n",
    "   ->  Breaks the text into words (.split()), removes extra whitespace, and joins it back with single spaces.\n",
    "   -> Reason : Handles multiple spaces or irregular spacing.\n",
    "   -> Purpose: Ensures consistent word separation and formatting.\n",
    "\n",
    "-> return text.lower()\n",
    "   -> Converts all characters in the text to lowercase.\n",
    "   -> Reason : To reduce vocabulary size. E.g., India and india should be treated the same.\n",
    "   -> Purpose: Simplifies training and improves model generalization.\n",
    "\n",
    "-> return \"\"\n",
    "  ->  If the input text is not a string, return an empty string.\n",
    "  -> Prevents the function from failing on None or non-text inputs.\n",
    "  -> Purpose: Robustness.\n",
    "\n",
    "-> data['content_text'] = data['content_text'].apply(clean_text) / data['generated_headline'] = data['generated_headline'].apply(clean_text)\n",
    "   -> Applies clean_text() to every row in the generated_headline column.\n",
    "   -> Reason : Prepares target output (headline) for model training.\n",
    "   -> Purpose: Ensures the decoder learns from clean data.\n",
    "\n",
    "-> data['generated_headline'] = data['generated_headline'].apply(lambda x: 'start ' + x + ' end')\n",
    "   -> Adds 'start ' at the beginning and ' end' at the end of every headline.\n",
    "   -> These special tokens help the model:Know when to start generating.Know when to stop predicting further words.\n",
    "   -> Purpose: This is essential for sequence-to-sequence models like LSTM/GRU with attention or greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1e0a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input and output sequences\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f02f9",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> import tensorflow\n",
    "   -> Imports the TensorFlow library.\n",
    "   -> Reason : TensorFlow is used to build and train deep learning models.\n",
    "   -> Purpose: Required for using Keras layers, preprocessing tools, and models.\n",
    "\n",
    "-> from tensorflow.keras.preprocessing.text import Tokenizer:Imports the Tokenizer class from Keras.\n",
    "   -> Reason : It tokenizes (converts) text into sequences of integers.\n",
    "   -> Purpose: To convert words to indices so that neural networks can process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc96292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit tokenizer on both inputs and outputs to build vocabulary\n",
    "tokenizer.fit_on_texts(x.tolist()+y.tolist())\n",
    "\n",
    "# Store word-to-index mapping\n",
    "index_word = tokenizer.word_index\n",
    "\n",
    "with open(\"tokenizer.pkl\",'wb') as file:\n",
    "    pickle.dump(tokenizer,file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742cbc16",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> tokenizer = Tokenizer() : Creates an instance of the Tokenizer.\n",
    "   -> Reason : Initializes an empty tokenizer object that will be fitted on your dataset.\n",
    "   -> Purpose: To build a vocabulary and prepare text sequences.\n",
    "\n",
    "-> tokenizer.fit_on_texts(x.tolist()+y.tolist()) : Fits the tokenizer on both input (x) and output (y) text.\n",
    "   -> Reason : Ensures that the vocabulary covers all words in the full dataset (articles + headlines).\n",
    "   -> Purpose: Builds a word_index mapping (e.g., \"hello\" → 5).\n",
    "\n",
    "-> index_word = tokenizer.word_index : Stores the word-to-index mapping dictionary from the tokenizer.\n",
    "   -> Reason : You may need it later for decoding predictions (converting back from tokens to words).\n",
    "   -> Purpose: To allow reverse lookup (e.g., for displaying generated text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a60a37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input and output texts to sequences of integers\n",
    "x_seq = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "y_seq = tokenizer.texts_to_sequences(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6640c48d",
   "metadata": {},
   "source": [
    "### Code Explanation : \n",
    "\n",
    "-> x_seq = tokenizer.texts_to_sequences(x) : Converts all input texts (x) into sequences of integers.\n",
    "   -> Reason : LSTMs and GRUs don’t process raw text — they need integer token inputs.\n",
    "   -> Purpose: To numerically represent input text for training.\n",
    "-> y_seq = tokenizer.texts_to_sequences(y) : Converts output texts (y, the headlines) into integer sequences.\n",
    "   -> Reason : Decoder input/output also needs to be integer-encoded.\n",
    "   -> Purpose: For the model to learn the mapping from input sequence to output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "232f3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine max sequence lengths for padding\n",
    "x_maxlen = max(len(seq) for seq in x_seq)\n",
    "y_maxlen = max(len(seq) for seq in y_seq)\n",
    "\n",
    "# Calculate total vocabulary size\n",
    "vocab_size = len(tokenizer.word_index)+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b50b79",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    " \n",
    "-> y_maxlen = max(len(seq) for seq in y_seq) : Finds the maximum length of output (headline) sequences.\n",
    "   -> Reason : Same reason as above — needed for padding.\n",
    "   -> Purpose: Ensures decoder inputs and outputs are the same length across batches.\n",
    "-> vocab_size = len(tokenizer.word_index)+1 : Calculates the total number of unique tokens + 1.\n",
    "   -> Reason : Tokenizer indices start from 1. So we add 1 to include a 0-padding token.\n",
    "   -> Purpose: Needed to set dimensions for the Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4436fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad input and output sequences to uniform length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "x_padded = pad_sequences(x_seq, maxlen=x_maxlen, padding='pre')\n",
    "y_padded = pad_sequences(y_seq, maxlen=y_maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a680e0",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> from tensorflow.keras.preprocessing.sequence import pad_sequences : Imports the function for padding sequences.\n",
    "   -> Reason : Required to make all sequences have the same length.\n",
    "   -> Purpose: Essential for batching inputs to the model.\n",
    "\n",
    "-> x_padded = pad_sequences(x_seq, maxlen=x_maxlen, padding='pre') : Pads input sequences on the left side ('pre') with zeros to make them equal \n",
    "              in length.\n",
    "   -> Reason : LSTMs work better when shorter sequences are padded from the beginning.\n",
    "   -> Purpose: Converts list of sequences into a matrix that can be fed to the encoder.\n",
    "\n",
    "-> y_padded = pad_sequences(y_seq, maxlen=y_maxlen, padding='post') : Pads target sequences (headlines) on the right side ('post').\n",
    "   -> Reason : Output sequences are often padded at the end so the model learns to generate tokens until it hits <end>.\n",
    "   -> Purpose: Prepares decoder input/output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "617c588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"X shape:\", x_padded.shape)\n",
    "#print(\"Y shape:\", y_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d2c533a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_padded,y_padded,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35275351",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> from sklearn.model_selection import train_test_split : Imports a utility to split the dataset into training and testing sets.\n",
    "   -> Reason : Essential for validating model performance on unseen data.\n",
    "   -> Purpose: Prevents overfitting by evaluating on test data.\n",
    "\n",
    "->  x_train, x_test, y_train, y_test = train_test_split(x_padded, y_padded, test_size=0.2, random_state=42)\n",
    "    -> Splits the data into 80% training and 20% testing.\n",
    "    -> Reason : test_size=0.2 ensures a fair split; random_state=42 ensures reproducibility.\n",
    "    -> Purpose: Prepares inputs/targets for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db430f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models, layers and callbacks\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Embedding, LSTM,GRU,Dense,TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8825f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare decoder inputs and targets\n",
    "# Decoder target is the output shifted by 1 word\n",
    "\n",
    "# For training\n",
    "decoder_input_train = y_train[:, :-1]\n",
    "decoder_output_train = y_train[:, 1:]\n",
    "\n",
    "# For testing\n",
    "decoder_input_test = y_test[:, :-1]\n",
    "decoder_output_test = y_test[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b08745",
   "metadata": {},
   "source": [
    "### Code Explanation : \n",
    "\n",
    "-> decoder_input_train = y_train[:, :-1] / decoder_input_test = y_test[:, :-1]\n",
    "   -> Takes all training target sequences (y_train) and removes the last token of each sequence.\n",
    "   -> Reason : This becomes the input to the decoder during training.The input to the decoder typically starts with the <start> token.\n",
    "-> Example:\n",
    "   -> If your y_train sequence is:\n",
    "      [start, the, president, speaks, end]\n",
    "      Then:\n",
    "   -> decoder_input_train = [start, the, president, speaks]\n",
    "   -> These are the tokens you feed into the decoder at each timestep to predict the next word.\n",
    "\n",
    "-> decoder_output_train = y_train[:, 1:] / decoder_output_test = y_test[:, 1:]\n",
    "   -> Takes all training target sequences (y_train) and removes the first token of each sequence.\n",
    "   -> Reason : This becomes the expected output that the decoder should predict at each timestep.\n",
    "               The decoder is trained to predict the next word based on previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c39b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape output to fit sparse_categorical_crossentropy\n",
    "decoder_output_train = decoder_output_train.reshape(\n",
    "    decoder_output_train.shape[0], decoder_output_train.shape[1], 1\n",
    ")\n",
    "decoder_output_test = decoder_output_test.reshape(\n",
    "    decoder_output_test.shape[0], decoder_output_test.shape[1], 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04fa8e1",
   "metadata": {},
   "source": [
    "### Code Explanation : \n",
    "\n",
    "-> decoder_output_train = decoder_output_train.reshape(\n",
    "   decoder_output_train.shape[0], decoder_output_train.shape[1], 1)\n",
    "-> Reshapes the decoder_output_train array from a 2D shape to a 3D shape.\n",
    " Before:\n",
    "-> decoder_output_train has shape:(num_samples, sequence_length)\n",
    "-> For example: (800, 15) → 800 sequences, each of length 15.\n",
    "After:\n",
    "-> It becomes:(num_samples, sequence_length, 1)\n",
    "-> For example: (800, 15, 1) → adds an extra dimension for the output.\n",
    "-> Reason : To make the label format compatible with the expected output shape of the model when using a sparse classification loss.\n",
    "\n",
    "-> ecoder_output_test = decoder_output_test.reshape(\n",
    "    decoder_output_test.shape[0], decoder_output_test.shape[1], 1)\n",
    "-> It reshapes the test target sequences in the same way.\n",
    "-> So during validation (with model.fit(..., validation_data=...)) you don’t get shape mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19b3b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding size and LSTM unit count\n",
    "embedding_dim = 100\n",
    "lstm_units = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "971d5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder: input -> embedding -> LSTM -> states\n",
    "encoder_inputs = Input(shape=(x_maxlen,))\n",
    "enc_emb = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = LSTM(lstm_units, return_state=True)(enc_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ae80b",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    " \n",
    "-> encoder_inputs = Input(shape=(x_maxlen,))\n",
    "   -> This creates a Keras Input layer to receive the encoder input sequence (i.e., the news article text).\n",
    "   -> shape=(x_maxlen,): The input is a sequence of integers (tokenized word IDs) of fixed length x_maxlen.\n",
    "   -> Reason : Every model in Keras starts with an input layer.\n",
    "               This is the placeholder for the source sequence (the news content you're summarizing into a headline).\n",
    "\n",
    "-> enc_emb = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "   -> Applies an Embedding layer to the input sequence.\n",
    "   -> Converts each word index in encoder_inputs into a dense vector of dimension embedding_dim.\n",
    "   -> Parameters:\n",
    "      -> vocab_size: The total number of words in your vocabulary.\n",
    "      -> embedding_dim: The number of dimensions for each word embedding (e.g., 100).\n",
    "   -> Reason : Neural networks don't understand raw word indices.\n",
    "               Embedding translates sparse word IDs into dense, trainable vectors that capture semantic meaning.\n",
    "\n",
    "-> encoder_outputs, state_h, state_c = LSTM(lstm_units, return_state=True)(enc_emb)\n",
    "   -> Feeds the embedded sequence into an LSTM layer.\n",
    "   -> Returns:\n",
    "             -> encoder_outputs: The full sequence of hidden states from the LSTM (not used in this basic model).\n",
    "             -> state_h: Final hidden state (important).\n",
    "             -> state_c: Final cell state (important).\n",
    "   -> Parameters:\n",
    "             -> lstm_units: Number of units (neurons) in the LSTM cell (e.g., 128).\n",
    "             -> return_state=True: This tells Keras to return the internal states (state_h and state_c), which are essential for the decoder.\n",
    "   -> Reason : \n",
    "             -> The LSTM encodes the entire input sequence into two vectors: state_h and state_c.\n",
    "             -> These vectors carry the context of the entire input and are passed to the decoder to help it generate a relevant output (headline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f200b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder: input -> embedding -> LSTM (uses encoder states) -> Dense\n",
    "decoder_inputs = Input(shape=(y_maxlen - 1,))\n",
    "dec_emb = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True,return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324850c",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "-> decoder_inputs = Input(shape=(y_maxlen - 1,))\n",
    "   -> Creates a Keras Input layer for the target sequence input.\n",
    "   -> y_maxlen - 1: Because during training, we shift the headline input and output by 1 time step.\n",
    "   -> The decoder input will be the headline with the <end> token removed.\n",
    "   -> The decoder output will be the headline with the <start> token removed.\n",
    "   -> Reason : The decoder must learn to predict the next word in the headline, given the previous words.\n",
    "               This is the input to the decoder during training, i.e., decoder_input_train.\n",
    "\n",
    "-> dec_emb = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "   -> Applies an Embedding layer to the decoder input tokens.\n",
    "   -> Converts each token (word index) into a dense embedding vector.\n",
    "   -> Reason : Just like the encoder, the decoder also needs semantic-rich vector inputs to feed into the LSTM.\n",
    "               To convert word IDs into embeddings so the LSTM can learn meaningful patterns and relationships between words.\n",
    "\n",
    "-> decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "   -> Initializes the LSTM layer for the decoder.\n",
    "   -> return_sequences=True: Returns the full sequence of hidden states (one per word).\n",
    "   -> return_state=True: Also returns the final hidden and cell states (optional here but useful for inference).\n",
    "   -> Reason : The decoder needs to process the entire input sequence and output a sequence of predictions (one per word).\n",
    "               This LSTM learns to generate a sequence of output words based on the encoder’s context and previous decoder outputs.\n",
    "\n",
    "-> decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "   -> Runs the embedded decoder inputs (dec_emb) through the LSTM decoder.\n",
    "   -> initial_state=[state_h, state_c]: Initializes the LSTM with the final hidden (state_h) and cell (state_c) states from the encoder.\n",
    "   -> _: We discard the final states for now (they’re not needed in training, but used during inference).\n",
    "   -> Reason : Passing encoder states here allows the decoder to \"know\" the context of the input sequence.\n",
    "               Without this, the decoder would not know what it's trying to generate a headline for.\n",
    "               To generate contextual hidden states in the decoder that are influenced by the original input content.\n",
    "            \n",
    "-> decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "   -> Creates a Dense output layer to convert the decoder’s LSTM output at each timestep into a probability distribution over the vocabulary.\n",
    "   -> activation='softmax': Converts raw scores into probabilities for each word in the vocabulary.\n",
    "   -> Reason : You need to predict the next word in the headline from all possible words in the vocabulary.\n",
    "               This layer allows the model to select the most likely next word during training or inference.\n",
    "\n",
    "-> decoder_outputs = decoder_dense(decoder_outputs)\n",
    "   -> Applies the Dense layer to the sequence output from the LSTM.\n",
    "   -> Converts the decoder’s LSTM output into a sequence of predicted word probabilities.\n",
    "   -> Reason : The model needs to output a word at each time step, and this gives the probability distribution over all possible words.\n",
    "               Final step in the decoder: turn the hidden states into actual word predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "96bb3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "headline_lstm_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "headline_lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4157de",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> headline_lstm_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "   -> Model(...): This is a function from Keras (tensorflow.keras.models.Model) used to define a complete model.\n",
    "   -> We have to give it :A list of input layers,An output layer.It then builds a model that maps the inputs to the outputs.\n",
    "   -> [encoder_inputs, decoder_inputs]:This is the input to the model and contains two parts:\n",
    "      -> encoder_inputs:This input layer receives the input text sequence (e.g., a news article or document).\n",
    "                        It’s passed through an Embedding layer and then into an LSTM encoder to generate context/state.\n",
    "      -> decoder_inputs:This input layer receives the target sequence during training (e.g., headline with <start> token).\n",
    "                        It also goes through an Embedding → LSTM decoder that uses the encoder’s states as initial state.\n",
    "      -> decoder_outputs:This is the final output of the decoder part of the network:\n",
    "                         It is a sequence of probability distributions (via softmax) over the vocabulary at each timestep.\n",
    "                         It predicts the next word in the output sequence.\n",
    "    -> Shape: (batch_size, y_maxlen - 1, vocab_size)\n",
    "-> headline_lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "   -> Compiles model using:\n",
    "      -> adam: efficient optimizer.\n",
    "      -> sparse_categorical_crossentropy: suitable loss for multi-class classification when labels are integers (not one-hot).\n",
    "   -> Enables the model to learn by minimizing prediction error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95f51813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to stop training early if no improvement\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ce53e5",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    " -> callbacks=[early_stopping]: stops training if no improvement in validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4ad4189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "25/25 [==============================] - 32s 295ms/step - loss: 5.7486 - accuracy: 0.1994 - val_loss: 3.5204 - val_accuracy: 0.1867\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 3s 108ms/step - loss: 2.9216 - accuracy: 0.2019 - val_loss: 2.6976 - val_accuracy: 0.2600\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 1s 57ms/step - loss: 2.5455 - accuracy: 0.2262 - val_loss: 2.4530 - val_accuracy: 0.2075\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 2s 91ms/step - loss: 2.3553 - accuracy: 0.2979 - val_loss: 2.2840 - val_accuracy: 0.3492\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 1s 55ms/step - loss: 2.1706 - accuracy: 0.3823 - val_loss: 2.0840 - val_accuracy: 0.4367\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 1s 50ms/step - loss: 1.9510 - accuracy: 0.4819 - val_loss: 1.8483 - val_accuracy: 0.5933\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 1s 61ms/step - loss: 1.7113 - accuracy: 0.6923 - val_loss: 1.6041 - val_accuracy: 0.7367\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 1s 49ms/step - loss: 1.4598 - accuracy: 0.7869 - val_loss: 1.3470 - val_accuracy: 0.7817\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 6s 261ms/step - loss: 1.1982 - accuracy: 0.8321 - val_loss: 1.0773 - val_accuracy: 0.8550\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 1s 52ms/step - loss: 0.9590 - accuracy: 0.8529 - val_loss: 0.8541 - val_accuracy: 0.8483\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 1s 42ms/step - loss: 0.7605 - accuracy: 0.8535 - val_loss: 0.6832 - val_accuracy: 0.8542\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 1s 43ms/step - loss: 0.6222 - accuracy: 0.8523 - val_loss: 0.5733 - val_accuracy: 0.8542\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 1s 43ms/step - loss: 0.5366 - accuracy: 0.8508 - val_loss: 0.5069 - val_accuracy: 0.8483\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 6s 256ms/step - loss: 0.4828 - accuracy: 0.8529 - val_loss: 0.4650 - val_accuracy: 0.8542\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 1s 48ms/step - loss: 0.4506 - accuracy: 0.8529 - val_loss: 0.4390 - val_accuracy: 0.8542\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 1s 39ms/step - loss: 0.4273 - accuracy: 0.8515 - val_loss: 0.4211 - val_accuracy: 0.8483\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 1s 40ms/step - loss: 0.4119 - accuracy: 0.8537 - val_loss: 0.4077 - val_accuracy: 0.8558\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 1s 39ms/step - loss: 0.4015 - accuracy: 0.8571 - val_loss: 0.4000 - val_accuracy: 0.8592\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 1s 38ms/step - loss: 0.3915 - accuracy: 0.8556 - val_loss: 0.3931 - val_accuracy: 0.8583\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 1s 49ms/step - loss: 0.3811 - accuracy: 0.8758 - val_loss: 0.3864 - val_accuracy: 0.8550\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 1s 39ms/step - loss: 0.3727 - accuracy: 0.8690 - val_loss: 0.3826 - val_accuracy: 0.8575\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 3s 105ms/step - loss: 0.3646 - accuracy: 0.8719 - val_loss: 0.3791 - val_accuracy: 0.8567\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 1s 41ms/step - loss: 0.3541 - accuracy: 0.8796 - val_loss: 0.3770 - val_accuracy: 0.8583\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.3459 - accuracy: 0.8760 - val_loss: 0.3766 - val_accuracy: 0.8583\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 2s 68ms/step - loss: 0.3369 - accuracy: 0.8771 - val_loss: 0.3769 - val_accuracy: 0.8558\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 1s 43ms/step - loss: 0.3253 - accuracy: 0.8865 - val_loss: 0.3756 - val_accuracy: 0.8600\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.3141 - accuracy: 0.8881 - val_loss: 0.3789 - val_accuracy: 0.8583\n",
      "Epoch 28/40\n",
      "25/25 [==============================] - 1s 41ms/step - loss: 0.3028 - accuracy: 0.8942 - val_loss: 0.3812 - val_accuracy: 0.8575\n",
      "Epoch 29/40\n",
      "25/25 [==============================] - 1s 40ms/step - loss: 0.2897 - accuracy: 0.9029 - val_loss: 0.3845 - val_accuracy: 0.8608\n",
      "Epoch 30/40\n",
      "25/25 [==============================] - 1s 39ms/step - loss: 0.2780 - accuracy: 0.8998 - val_loss: 0.3934 - val_accuracy: 0.8567\n",
      "Epoch 31/40\n",
      "25/25 [==============================] - 4s 165ms/step - loss: 0.2662 - accuracy: 0.9104 - val_loss: 0.4050 - val_accuracy: 0.8600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1be47c51db0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "headline_lstm_model.fit(\n",
    "    [x_train, decoder_input_train],\n",
    "    decoder_output_train,\n",
    "    validation_data=([x_test, decoder_input_test], decoder_output_test),\n",
    "    batch_size=32,epochs=40,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b0f279",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> headline_lstm_model.fit(\n",
    "    [x_train, decoder_input_train],decoder_output_train,validation_data=([x_test, decoder_input_test], decoder_output_test),batch_size=32, epochs=40,callbacks=[early_stopping])\n",
    "   -> Key components:\n",
    "      -> x_train: input articles (tokenized, padded).\n",
    "      -> decoder_input_train: shifted target sequences (with <start> token).\n",
    "      -> decoder_output_train: expected output sequences (with <end> token), reshaped for training.\n",
    "      -> validation_data: evaluates the model on unseen test data during training.\n",
    "      -> batch_size=32: processes 32 samples at a time.\n",
    "      -> epochs=40: trains for up to 40 epochs.\n",
    "      -> callbacks=[early_stopping]: stops training if no improvement in validation loss.\n",
    "   -> Reason : Trains the full sequence-to-sequence model so that, given input text, it can learn to generate the correct output sequence \n",
    "                 (e.g.headline).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7408754d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GEN_AI\\A_neural\\.venv\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Save the model\n",
    "headline_lstm_model.save(\"headline_lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e7e80d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "25/25 [==============================] - 16s 184ms/step - loss: 6.3152 - accuracy: 0.1981 - val_loss: 3.5069 - val_accuracy: 0.1667\n",
      "Epoch 2/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 3.0387 - accuracy: 0.2081 - val_loss: 2.8001 - val_accuracy: 0.3075\n",
      "Epoch 3/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 2.6073 - accuracy: 0.2600 - val_loss: 2.4653 - val_accuracy: 0.2875\n",
      "Epoch 4/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 2.2868 - accuracy: 0.3617 - val_loss: 2.1599 - val_accuracy: 0.3825\n",
      "Epoch 5/40\n",
      "25/25 [==============================] - 1s 32ms/step - loss: 1.9931 - accuracy: 0.4727 - val_loss: 1.8608 - val_accuracy: 0.5133\n",
      "Epoch 6/40\n",
      "25/25 [==============================] - 1s 31ms/step - loss: 1.6965 - accuracy: 0.6108 - val_loss: 1.5505 - val_accuracy: 0.7858\n",
      "Epoch 7/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 1.3596 - accuracy: 0.8052 - val_loss: 1.1651 - val_accuracy: 0.8542\n",
      "Epoch 8/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.9544 - accuracy: 0.8527 - val_loss: 0.7603 - val_accuracy: 0.8533\n",
      "Epoch 9/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.6292 - accuracy: 0.8529 - val_loss: 0.5312 - val_accuracy: 0.8542\n",
      "Epoch 10/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.4790 - accuracy: 0.8508 - val_loss: 0.4458 - val_accuracy: 0.8567\n",
      "Epoch 11/40\n",
      "25/25 [==============================] - 2s 91ms/step - loss: 0.4242 - accuracy: 0.8562 - val_loss: 0.4087 - val_accuracy: 0.8542\n",
      "Epoch 12/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.4008 - accuracy: 0.8515 - val_loss: 0.3917 - val_accuracy: 0.8542\n",
      "Epoch 13/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.3866 - accuracy: 0.8525 - val_loss: 0.3855 - val_accuracy: 0.8483\n",
      "Epoch 14/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.3829 - accuracy: 0.8533 - val_loss: 0.3754 - val_accuracy: 0.8542\n",
      "Epoch 15/40\n",
      "25/25 [==============================] - 2s 91ms/step - loss: 0.3731 - accuracy: 0.8550 - val_loss: 0.3712 - val_accuracy: 0.8567\n",
      "Epoch 16/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.3686 - accuracy: 0.8556 - val_loss: 0.3674 - val_accuracy: 0.8542\n",
      "Epoch 17/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.3658 - accuracy: 0.8533 - val_loss: 0.3641 - val_accuracy: 0.8517\n",
      "Epoch 18/40\n",
      "25/25 [==============================] - 1s 36ms/step - loss: 0.3645 - accuracy: 0.8525 - val_loss: 0.3636 - val_accuracy: 0.8567\n",
      "Epoch 19/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.3618 - accuracy: 0.8548 - val_loss: 0.3598 - val_accuracy: 0.8542\n",
      "Epoch 20/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.3600 - accuracy: 0.8548 - val_loss: 0.3604 - val_accuracy: 0.8525\n",
      "Epoch 21/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.3565 - accuracy: 0.8567 - val_loss: 0.3594 - val_accuracy: 0.8533\n",
      "Epoch 22/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.3544 - accuracy: 0.8617 - val_loss: 0.3558 - val_accuracy: 0.8558\n",
      "Epoch 23/40\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.3504 - accuracy: 0.8646 - val_loss: 0.3582 - val_accuracy: 0.8492\n",
      "Epoch 24/40\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.3419 - accuracy: 0.8706 - val_loss: 0.3566 - val_accuracy: 0.8542\n",
      "Epoch 25/40\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.3279 - accuracy: 0.8779 - val_loss: 0.3602 - val_accuracy: 0.8558\n",
      "Epoch 26/40\n",
      "25/25 [==============================] - 2s 75ms/step - loss: 0.3154 - accuracy: 0.8900 - val_loss: 0.3578 - val_accuracy: 0.8583\n",
      "Epoch 27/40\n",
      "25/25 [==============================] - 2s 66ms/step - loss: 0.3046 - accuracy: 0.8998 - val_loss: 0.3642 - val_accuracy: 0.8567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1be4249ee30>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRU Encoder\n",
    "encoder_inputs_gru = Input(shape=(x_maxlen,))\n",
    "enc_emb_gru = Embedding(vocab_size, embedding_dim)(encoder_inputs_gru)\n",
    "encoder_outputs_gru, state_gru = GRU(lstm_units, return_state=True)(enc_emb_gru)\n",
    "\n",
    "# GRU Decoder\n",
    "decoder_inputs_gru = Input(shape=(y_maxlen - 1,))\n",
    "dec_emb_gru = Embedding(vocab_size, embedding_dim)(decoder_inputs_gru)\n",
    "decoder_gru = GRU(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs_gru, _ = decoder_gru(dec_emb_gru, initial_state=state_gru)\n",
    "decoder_dense_gru = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs_gru = decoder_dense_gru(decoder_outputs_gru)\n",
    "\n",
    "# GRU Model\n",
    "gru_model = Model([encoder_inputs_gru, decoder_inputs_gru], decoder_outputs_gru)\n",
    "gru_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train GRU model\n",
    "gru_model.fit(\n",
    "    [x_train, decoder_input_train],\n",
    "    decoder_output_train,\n",
    "    validation_data=([x_test, decoder_input_test], decoder_output_test),\n",
    "    batch_size=32,epochs=40,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7e0d95",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> encoder_inputs_gru = Input(shape=(x_maxlen,))\n",
    "   -> Creates an input layer for the encoder with shape (x_maxlen,), meaning a sequence of word indices of max length x_maxlen.\n",
    "   -> Reason : The encoder takes an input sequence (e.g. cleaned article text) as a series of tokens (integers).\n",
    "\n",
    "-> enc_emb_gru = Embedding(vocab_size, embedding_dim)(encoder_inputs_gru)\n",
    "   -> Embedding layer turns the integer tokens into dense word vectors of size embedding_dim.\n",
    "   -> Reason : Embeddings give semantic meaning to tokens for better GRU learning.\n",
    "\n",
    "-> encoder_outputs_gru, state_gru = GRU(lstm_units, return_state=True)(enc_emb_gru)\n",
    "   -> Passes embeddings to a GRU layer.\n",
    "   -> return_state=True: We return the final hidden state state_gru, which summarizes the input sequence.\n",
    "   -> Reason : This state is passed to the decoder as initial context.\n",
    "\n",
    "-> decoder_inputs_gru = Input(shape=(y_maxlen - 1,))\n",
    "   -> Input layer for the decoder, which takes target sequences shifted right (i.e., without the <end> token).\n",
    "   -> Reason : Used during training; model learns to predict the next word from previous ones.\n",
    "\n",
    "-> dec_emb_gru = Embedding(vocab_size, embedding_dim)(decoder_inputs_gru)\n",
    "   -> Embedding layer for decoder inputs.\n",
    "   -> Reason : Converts output tokens into embeddings for the decoder GRU to understand.\n",
    "\n",
    "-> decoder_gru = GRU(lstm_units, return_sequences=True, return_state=True)\n",
    "   -> Initializes a GRU layer for the decoder.\n",
    "   -> return_sequences=True: Needed because the decoder must output a sequence (not a single output).\n",
    "   -> Reason : To generate a word at each step of the target sequence.\n",
    "\n",
    "-> decoder_outputs_gru, _ = decoder_gru(dec_emb_gru, initial_state=state_gru)\n",
    "   -> Feeds embedded target sequence to the decoder GRU.\n",
    "   -> initial_state=state_gru: Uses the final state from the encoder GRU to guide decoding.\n",
    "   -> Reason : This provides the decoder with context about the input sequence.\n",
    "\n",
    "-> decoder_dense_gru = Dense(vocab_size, activation='softmax')\n",
    "   -> Dense layer that predicts the next word (probability distribution over vocabulary).\n",
    "   -> Reason : Converts GRU output at each time step into a probability over words.\n",
    "\n",
    "-> decoder_outputs_gru = decoder_dense_gru(decoder_outputs_gru)\n",
    "   -> Applies the dense layer to GRU outputs.\n",
    "   ->  Produces final predicted word sequence.\n",
    "\n",
    "-> GRU Model Compilation\n",
    "  -> gru_model = Model([encoder_inputs_gru, decoder_inputs_gru], decoder_outputs_gru)\n",
    "  -> Defines the final Keras model, connecting encoder and decoder inputs to decoder outputs.\n",
    "  -> Reason : This is the full model to be trained.\n",
    "\n",
    "-> gru_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "   -> Compiles model using:\n",
    "      -> adam: efficient optimizer.\n",
    "      -> sparse_categorical_crossentropy: suitable loss for multi-class classification when labels are integers (not one-hot).\n",
    "   -> Enables the model to learn by minimizing prediction error.\n",
    "\n",
    "-> gru_model.fit(\n",
    "    [x_train, decoder_input_train],decoder_output_train,validation_data=([x_test, decoder_input_test], decoder_output_test),batch_size=32, epochs=40,callbacks=[early_stopping])\n",
    "   -> Key components:\n",
    "      -> x_train: input articles (tokenized, padded).\n",
    "      -> decoder_input_train: shifted target sequences (with <start> token).\n",
    "      -> decoder_output_train: expected output sequences (with <end> token), reshaped for training.\n",
    "      -> validation_data: evaluates the model on unseen test data during training.\n",
    "      -> batch_size=32: processes 32 samples at a time.\n",
    "      -> epochs=40: trains for up to 40 epochs.\n",
    "      -> callbacks=[early_stopping]: stops training if no improvement in validation loss.\n",
    "   -> Reason : Trains the full sequence-to-sequence model so that, given input text, it can learn to generate the correct output sequence \n",
    "                 (e.g.headline).\n",
    "\n",
    "-> Summary Diagram:\n",
    "Input (x_train) ───> Encoder GRU ───┐\n",
    "                                    │\n",
    "Target (decoder_input_train) ──> Decoder GRU ──> Dense (softmax) ──> Output headline\n",
    "                                    │\n",
    "                          (Initial state from Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d0d1cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GRU Model\n",
    "gru_model.save(\"headline_gru_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "85cf9a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the numpy library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e5f4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Inference Models -------------------\n",
    "# LSTM inference\n",
    "encoder_model_lstm = Model(encoder_inputs, [state_h, state_c])\n",
    "\n",
    "decoder_state_input_h = Input(shape=(lstm_units,))\n",
    "decoder_state_input_c = Input(shape=(lstm_units,))\n",
    "decoder_hidden_inputs = Input(shape=(1,))\n",
    "decoder_emb_infer = Embedding(vocab_size, embedding_dim)(decoder_hidden_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_emb_infer,\n",
    "                                                    initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model_lstm = Model(\n",
    "    [decoder_hidden_inputs, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2, state_h2, state_c2]\n",
    ")\n",
    "\n",
    "# GRU inference\n",
    "encoder_model_gru = Model(encoder_inputs_gru, state_gru)\n",
    "\n",
    "decoder_state_input_gru = Input(shape=(lstm_units,))\n",
    "decoder_hidden_inputs_gru = Input(shape=(1,))\n",
    "decoder_emb_infer_gru = Embedding(vocab_size, embedding_dim)(decoder_hidden_inputs_gru)\n",
    "decoder_outputs_gru_inf, state_gru_inf = decoder_gru(decoder_emb_infer_gru, initial_state=decoder_state_input_gru)\n",
    "decoder_outputs_gru_inf = decoder_dense_gru(decoder_outputs_gru_inf)\n",
    "decoder_model_gru = Model(\n",
    "    [decoder_hidden_inputs_gru, decoder_state_input_gru],\n",
    "    [decoder_outputs_gru_inf, state_gru_inf]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7451f71",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    " #### Why do we need inference models separately?\n",
    "-> During training, the encoder and decoder run together using full sequences.\n",
    "-> During inference (prediction), we:\n",
    "    -> Encode the input once,\n",
    "    -> Then decode one word at a time, feeding each predicted word back to the decoder to get the next word.\n",
    "    -> So, we rebuild smaller versions of the models for this step. That’s what you're doing here.\n",
    "\n",
    "-> encoder_model_lstm = Model(encoder_inputs, [state_h, state_c])\n",
    "    -> Creates an encoder model for inference using LSTM.\n",
    "    -> encoder_inputs: The input layer (tokenized padded input text).\n",
    "    -> [state_h, state_c]: Outputs only the hidden and cell states (not the whole sequence).\n",
    "    -> Reason : During inference, we only need the final states from the encoder to pass into the decoder.\n",
    "\n",
    "-> decoder_state_input_h = Input(shape=(lstm_units,))\n",
    "   decoder_state_input_c = Input(shape=(lstm_units,))\n",
    "   -> Defines input layers for the decoder's previous timestep hidden state (h) and cell state (c).\n",
    "   -> Reason : In inference, we decode one word at a time — we must feed the previous LSTM states back into the decoder.\n",
    "\n",
    "-> decoder_hidden_inputs = Input(shape=(1,))\n",
    "   -> Defines an input layer for the decoder that will take one token at a time (e.g., 'start', 'the', etc.).\n",
    "   -> Reason : Unlike training (where we pass full sequences), inference works step-by-step. So, we feed just one word each time.\n",
    "\n",
    "-> decoder_emb_infer = Embedding(vocab_size, embedding_dim)(decoder_hidden_inputs)\n",
    "   -> Embeds the one-word input into a dense vector using the same Embedding layer setup.\n",
    "   -> Reason : The decoder expects embedded vectors, not plain integers. This gives word representations learned during training.\n",
    "\n",
    "-> decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
    "   decoder_emb_infer, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "   -> Runs the decoder LSTM for 1 time step, using:\n",
    "   -> The embedded input word,\n",
    "   -> The previous hidden and cell states.\n",
    "   -> Returns:\n",
    "      -> Output for the current step,\n",
    "      -> Updated hidden and cell states (state_h2, state_c2).\n",
    "   -> Reason : We use these updated states for the next decoding step.\n",
    "\n",
    "-> decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "   -> Passes the decoder's output through the final Dense layer with softmax.\n",
    "   -> Reason : To get the probability distribution over all vocabulary words — from which we select the most probable one.\n",
    "\n",
    "-> decoder_model_lstm = Model(\n",
    "    [decoder_hidden_inputs, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2, state_h2, state_c2])\n",
    "   -> Creates the final LSTM decoder inference model.\n",
    "      -> Inputs : Current word,Previous state_h,Previous state_c\n",
    "      -> Outputs : Word prediction,Updated state_h,Updated state_c\n",
    "   -> Reason : This model allows step-by-step decoding, reusing the updated states every time.\n",
    "\n",
    "-> GRU Inference\n",
    "   -> Now, I replicate the same logic using GRU, which is simpler (only one hidden state).\n",
    "\n",
    "-> encoder_model_gru = Model(encoder_inputs_gru, state_gru)\n",
    "   -> Creates the encoder inference model for GRU.\n",
    "   -> Reason : The GRU encoder only outputs one state — state_gru.\n",
    "\n",
    "-> decoder_state_input_gru = Input(shape=(lstm_units,))\n",
    "   -> Defines the input for the GRU's previous hidden state.\n",
    "\n",
    "-> decoder_hidden_inputs_gru = Input(shape=(1,))\n",
    "   -> Defines an input for the one-token input word for the decoder\n",
    "\n",
    "-> decoder_emb_infer_gru = Embedding(vocab_size, embedding_dim)(decoder_hidden_inputs_gru)\n",
    "   -> Embeds the decoder input word into dense vectors.\n",
    "\n",
    "-> decoder_outputs_gru_inf, state_gru_inf = decoder_gru(\n",
    "    decoder_emb_infer_gru, initial_state=decoder_state_input_gru)\n",
    "   -> Feeds the input into the GRU decoder with previous state → gets:\n",
    "      -> Output word prediction\n",
    "      -> Updated GRU state\n",
    "\n",
    "-> decoder_outputs_gru_inf = decoder_dense_gru(decoder_outputs_gru_inf)\n",
    "   -> Passes decoder output through the final Dense layer to get vocabulary probabilities.\n",
    "\n",
    "-> decoder_model_gru = Model(\n",
    "    [decoder_hidden_inputs_gru, decoder_state_input_gru],\n",
    "    [decoder_outputs_gru_inf, state_gru_inf])\n",
    "   -> Creates the final GRU decoder model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2ca64850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate headline given input text\n",
    "def generate_headline(text, encoder_model, decoder_model, tokenizer, max_len, y_maxlen, is_lstm=True):\n",
    "    # Clean and tokenize the input\n",
    "    input_seq = tokenizer.texts_to_sequences([clean_text(text)])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_len, padding='pre')\n",
    "\n",
    "    # Encode input sequence\n",
    "    if is_lstm:\n",
    "        state_h, state_c = encoder_model.predict(input_seq, verbose=0)\n",
    "        states = [state_h, state_c]\n",
    "    else:\n",
    "        state_gru = encoder_model.predict(input_seq, verbose=0)\n",
    "        states = [state_gru]\n",
    "\n",
    "    # Start decoding with <start> token\n",
    "    start_token_id = tokenizer.word_index.get('start')\n",
    "    if not start_token_id:\n",
    "        raise ValueError(\"The tokenizer does not contain a 'start' token.\")\n",
    "\n",
    "    target_seq = np.array([[start_token_id]])\n",
    "    result = []\n",
    "\n",
    "    for _ in range(y_maxlen):\n",
    "        if is_lstm:\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq] + states, verbose=0)\n",
    "            states = [h, c]\n",
    "        else:\n",
    "            output_tokens, new_state = decoder_model.predict([target_seq] + states, verbose=0)\n",
    "            states = [new_state]\n",
    "\n",
    "        predicted_id = np.argmax(output_tokens[0, -1, :])\n",
    "        word = tokenizer.index_word.get(predicted_id, '')\n",
    "\n",
    "        if word == 'end' or word == '':\n",
    "            break\n",
    "\n",
    "        result.append(word)\n",
    "        target_seq = np.array([[predicted_id]])\n",
    "\n",
    "    return ' '.join(result).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f40181",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "-> def generate_headline(text, encoder_model, decoder_model, tokenizer, max_len, y_maxlen, is_lstm=True):Defines a user defined  function to generate a headline for a given news paragraph using a trained encoder-decoder model (LSTM or GRU).\n",
    "\n",
    "-> Parameters:\n",
    "             -> text: The raw news paragraph to summarize.\n",
    "\n",
    "             -> encoder_model: The inference model for encoding the input sequence.\n",
    "\n",
    "             -> decoder_model: The inference model for decoding/generating the headline.\n",
    "\n",
    "             -> tokenizer: Maps between words and integer tokens.\n",
    "\n",
    "             -> max_len: Maximum length for input padding.\n",
    "\n",
    "             --> y_maxlen: Maximum length of output (i.e., how long the headline should be).\n",
    "\n",
    "             -> is_lstm: Boolean flag to switch between LSTM and GRU inference models.\n",
    "\n",
    "-> input_seq = tokenizer.texts_to_sequences([clean_text(text)]):\n",
    "    -> Cleans the text using the same cleaning function used during training.\n",
    "    -> Converts the cleaned text into a list of integers (tokens) using the tokenizer.\n",
    "    -> Reason : Models only understand numbers. You must transform words into token IDs the model saw during training.\n",
    "\n",
    "-> input_seq = pad_sequences(input_seq, maxlen=max_len, padding='pre'):\n",
    "    -> Pads the tokenized input so it's exactly max_len long by adding zeros at the beginning.\n",
    "    -> Reason : The encoder expects input of a fixed length. Padding ensures consistency. \n",
    "\n",
    "-> if is_lstm:\n",
    "        state_h, state_c = encoder_model.predict(input_seq, verbose=0)\n",
    "        states = [state_h, state_c]\n",
    "    -> If using LSTM:\n",
    "    -> Pass the input to the encoder model.\n",
    "    -> Receive two states: hidden (state_h) and cell (state_c) state.\n",
    "    -> Store them in states.\n",
    "    -> Reason : LSTM maintains both hidden and cell states for better long-term memory tracking during decoding.\n",
    "\n",
    "-> else:\n",
    "        state_gru = encoder_model.predict(input_seq, verbose=0)\n",
    "        states = [state_gru]\n",
    "    -> If using GRU:\n",
    "    -> Pass the input to the encoder model.\n",
    "    -> GRU returns only one hidden state.\n",
    "    -> Reason : GRUs are simpler than LSTMs; they only return a single state.\n",
    "\n",
    "-> start_token_id = tokenizer.word_index.get('start'):\n",
    "    -> Gets the integer token that corresponds to the word 'start'.\n",
    "    -> Reason : This token is used to kick off the decoding process, telling the model: \"Start generating the headline now.\"\n",
    "\n",
    "-> if not start_token_id:\n",
    "        raise ValueError(\"The tokenizer does not contain a 'start' token.\")\n",
    "    -> Safety check to make sure 'start' token exists in the tokenizer.\n",
    "    -> Reason : Without the 'start' token, the model won't know where to begin generating output.\n",
    "\n",
    "-> target_seq = np.array([[start_token_id]])\n",
    "    -> Initializes the target input for the decoder with the <start> token.\n",
    "    -> Reason : This is the very first input to the decoder so it starts generating the first word.\n",
    "\n",
    "-> result = []\n",
    "    -> An empty list to store predicted words as they are generated.\n",
    "    -> Reason : This will ultimately contain the generated headline, word by word.\n",
    "\n",
    "-> for _ in range(y_maxlen):\n",
    "    -> Loop up to y_maxlen times to generate each word of the headline.\n",
    "    -> Reason : Headlines have a maximum length. Looping ensures we don't generate endlessly.\n",
    "\n",
    "->  if is_lstm:\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq] + states, verbose=0)\n",
    "            states = [h, c]\n",
    "    -> Pass the current input word (target_seq) and states to the decoder.\n",
    "    -> Get output token probabilities and updated states.\n",
    "    -> Update the states to use in the next step.\n",
    "    -> Reason : Each decoding step needs the previous state to generate the next word in sequence.\n",
    "\n",
    "-> else:\n",
    "            output_tokens, new_state = decoder_model.predict([target_seq] + states, verbose=0)\n",
    "            states = [new_state]\n",
    "    -> Same as above, but only one state is returned and updated.\n",
    "\n",
    "-> predicted_id = np.argmax(output_tokens[0, -1, :])\n",
    "    -> Finds the index (word ID) of the most probable word in the decoder output.\n",
    "    -> Reason : The decoder outputs a probability distribution over all vocabulary. argmax selects the most likely next word.\n",
    "\n",
    "-> word = tokenizer.index_word.get(predicted_id, '')\n",
    "    -> Converts the predicted token ID back to its corresponding word.\n",
    "    -> Reason : We need the actual word, not just the index, to append to the final result.\n",
    "\n",
    "-> if word == 'end' or word == '':\n",
    "            break\n",
    "    -> Stops generation if the model predicts the 'end' token or can't find a valid word.\n",
    "    -> Reason : 'end' indicates the end of the sequence. Empty predictions are treated as errors or unknowns.\n",
    "\n",
    "-> result.append(word)\n",
    "    -> Adds the predicted word to the output list.\n",
    "    -> To build the full sentence word by word.\n",
    "\n",
    "-> target_seq = np.array([[predicted_id]])\n",
    "     -> Prepares the predicted word as the next input to the decoder.\n",
    "     -> Reason : Decoding is sequential: the output at time t becomes input at time t+1.\n",
    "\n",
    "-> return ' '.join(result).strip()\n",
    "     -> Combines all predicted words into a single string and removes leading/trailing spaces.\n",
    "     -> Reason : Returns the final readable headline as a properly formatted string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f1f9eb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headline Predictions:\n",
      "\n",
      "1. Input Text    : The president addressed the media regarding the economic reforms.\n",
      "   LSTM Headline: nasa change wins\n",
      "   GRU Headline : ai discovers effects\n",
      "--------------------------------------------------------------------------------\n",
      "2. Input Text    : India launched a new satellite into orbit for communication services.\n",
      "   LSTM Headline: climate discovers wins championship\n",
      "   GRU Headline : ai discovers effects\n",
      "--------------------------------------------------------------------------------\n",
      "3. Input Text    : The finance minister presented the annual budget in parliament.\n",
      "   LSTM Headline: climate discovers wins championship\n",
      "   GRU Headline : ai discovers effects\n",
      "--------------------------------------------------------------------------------\n",
      "4. Input Text    : Heavy rains caused flooding in several districts across the state.\n",
      "   LSTM Headline: new team wins\n",
      "   GRU Headline : ai discovers effects\n",
      "--------------------------------------------------------------------------------\n",
      "5. Input Text    : The cricket team celebrated after winning the international tournament.\n",
      "   LSTM Headline: climate discovers wins championship\n",
      "   GRU Headline : ai discovers effects\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test Generation\n",
    "test_paragraphs = [\n",
    "    \"The president addressed the media regarding the economic reforms.\",\n",
    "    \"India launched a new satellite into orbit for communication services.\",\n",
    "    \"The finance minister presented the annual budget in parliament.\",\n",
    "    \"Heavy rains caused flooding in several districts across the state.\",\n",
    "    \"The cricket team celebrated after winning the international tournament.\"\n",
    "]\n",
    "\n",
    "print(\"\\nHeadline Predictions:\\n\")\n",
    "for i, para in enumerate(test_paragraphs, 1):\n",
    "    lstm_headline = generate_headline(para, encoder_model_lstm, decoder_model_lstm,\n",
    "                                      tokenizer, x_maxlen, y_maxlen, is_lstm=True)\n",
    "    gru_headline = generate_headline(para, encoder_model_gru, decoder_model_gru,\n",
    "                                     tokenizer, x_maxlen, y_maxlen, is_lstm=False)\n",
    "\n",
    "    print(f\"{i}. Input Text    : {para}\")\n",
    "    print(f\"   LSTM Headline: {lstm_headline}\")\n",
    "    print(f\"   GRU Headline : {gru_headline}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae50d5e",
   "metadata": {},
   "source": [
    "### Code Explanation :\n",
    "\n",
    "-> for i, para in enumerate(test_paragraphs, 1) : Loops over each paragraph (para) and assigns an index (i), starting from 1.\n",
    "   -> Reason : So you can number the output predictions (1., 2., etc.).\n",
    "   -> Purpose : Allows systematic evaluation and display of results for each test case.\n",
    "\n",
    "-> lstm_headline = generate_headline(para,encoder_model_lstm,decoder_model_lstm,tokenizer,x_maxlen,y_maxlen,is_lstm=True)\n",
    "   -> Generates a headline using the LSTM-based model for the current input paragraph.\n",
    "   -> Reason : Tests the LSTM model’s ability to summarize the input.\n",
    "   -> Purpose:Compares the LSTM model’s predictions with the GRU model for the same paragraph.\n",
    "\n",
    "-> gru_headline = generate_headline(para,encoder_model_gru,decoder_model_gru,tokenizer,x_maxlen,y_maxlen,is_lstm=False)\n",
    "   -> Generates a headline using the GRU-based model for the same paragraph.\n",
    "   -> Reason : To compare how differently the GRU model performs on the same input.\n",
    "   -> Purpose : Allows side-by-side evaluation of two architectures: LSTM vs GRU.\n",
    "\n",
    "-> print(f\"{i}. Input Text : {para}\") : Prints the original input paragraph.\n",
    "   -> Reason : Shows what the model is trying to summarize.\n",
    "   -> Purpose : Useful for human evaluation — we want to see if the generated headline makes sense.\n",
    "\n",
    "-> print(f\" LSTM Headline: {lstm_headline}\") : Prints the headline generated by the LSTM model.\n",
    "   -> Reason : To visually display and evaluate LSTM model predictions.\n",
    "   -> Purpose : Compare its summary quality with the GRU’s output.\n",
    "\n",
    "-> print(f\" GRU Headline : {gru_headline}\") : Prints the headline generated by the GRU model.\n",
    "   -> Reason : Completes the comparison between both models.\n",
    "   -> Purpose : Helps you decide which architecture is performing better in headline generation.\n",
    "\n",
    "-> print(\"-\" * 80) : Prints a horizontal line separator.\n",
    "   -> Reason : Visually separates results for each input paragraph.\n",
    "   -> Purpose : Improves readability of the console output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee6f0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_headline(text, encoder_model, decoder_model, tokenizer, max_len, y_maxlen, is_lstm=True):\n",
    "    # Clean and tokenize the input\n",
    "    input_seq = tokenizer.texts_to_sequences([clean_text(text)])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_len, padding='pre')\n",
    "\n",
    "    # Encode the input sequence\n",
    "    if is_lstm:\n",
    "        state_h, state_c = encoder_model.predict(input_seq)\n",
    "        states = [state_h, state_c]\n",
    "    else:\n",
    "        state_gru = encoder_model.predict(input_seq)\n",
    "        states = [state_gru]\n",
    "\n",
    "    # Start decoding with the <start> token\n",
    "    target_seq = np.array([[tokenizer.word_index['start']]])\n",
    "    result = []\n",
    "\n",
    "    for _ in range(y_maxlen):\n",
    "        if is_lstm:\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq] + states)\n",
    "            states = [h, c]\n",
    "        else:\n",
    "            output_tokens, new_state = decoder_model.predict([target_seq] + states)\n",
    "            states = [new_state]\n",
    "\n",
    "        predicted_id = np.argmax(output_tokens[0, -1, :])\n",
    "        word = tokenizer.index_word.get(predicted_id, '')\n",
    "\n",
    "        # Stop if 'end' or empty string is predicted\n",
    "        if word == 'end' or word == '':\n",
    "            break\n",
    "\n",
    "        result.append(word)\n",
    "        target_seq = np.array([[predicted_id]])\n",
    "\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b10e17ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headline Predictions:\n",
      "\n",
      "1/1 [==============================] - 1s 798ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Input Text      : The president addressed the media regarding the economic reforms.\n",
      "LSTM Headline   : schools change effects\n",
      "GRU Headline    : ai discovers hits learning\n",
      "------------------------------------------------------------\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 332ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Input Text      : India launched a new satellite into orbit for communication services.\n",
      "LSTM Headline   : stock change effects\n",
      "GRU Headline    : climate discovers hits\n",
      "------------------------------------------------------------\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Input Text      : The finance minister presented the annual budget in parliament.\n",
      "LSTM Headline   : new breakthrough wins\n",
      "GRU Headline    : climate discovers hits\n",
      "------------------------------------------------------------\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Input Text      : Heavy rains caused flooding in several districts across the state.\n",
      "LSTM Headline   : local discovers wins\n",
      "GRU Headline    : climate discovers hits learning\n",
      "------------------------------------------------------------\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Input Text      : The cricket team celebrated after winning the international tournament.\n",
      "LSTM Headline   : stock change effects\n",
      "GRU Headline    : climate discovers hits\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_paragraphs = [\n",
    "    \"The president addressed the media regarding the economic reforms.\",\n",
    "    \"India launched a new satellite into orbit for communication services.\",\n",
    "    \"The finance minister presented the annual budget in parliament.\",\n",
    "    \"Heavy rains caused flooding in several districts across the state.\",\n",
    "    \"The cricket team celebrated after winning the international tournament.\"\n",
    "]\n",
    "\n",
    "print(\"\\nHeadline Predictions:\\n\")\n",
    "for para in test_paragraphs:\n",
    "    lstm_headline = generate_headline(\n",
    "        para, encoder_model_lstm, decoder_model_lstm, tokenizer, x_maxlen, y_maxlen, is_lstm=True)\n",
    "    gru_headline = generate_headline(\n",
    "        para, encoder_model_gru, decoder_model_gru, tokenizer, x_maxlen, y_maxlen, is_lstm=False)\n",
    "    \n",
    "    print(f\"Input Text      : {para}\")\n",
    "    print(f\"LSTM Headline   : {lstm_headline}\")\n",
    "    print(f\"GRU Headline    : {gru_headline}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cdc6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Generate Headline Function -------------------\n",
    "def generate_headline(text, encoder_model, decoder_model, tokenizer, max_len, is_lstm=True):\n",
    "    input_seq = tokenizer.texts_to_sequences([clean_text(text)])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_len, padding='pre')\n",
    "\n",
    "    if is_lstm:\n",
    "        state_h, state_c = encoder_model.predict(input_seq)\n",
    "    else:\n",
    "        state_gru = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.array([[tokenizer.word_index['start']]])\n",
    "    result = []\n",
    "\n",
    "    for _ in range(y_maxlen):\n",
    "        if is_lstm:\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq, state_h, state_c])\n",
    "            state_h, state_c = h, c\n",
    "        else:\n",
    "            output_tokens, state_gru = decoder_model.predict([target_seq, state_gru])\n",
    "\n",
    "        predicted_id = np.argmax(output_tokens[0, -1, :])\n",
    "        word = tokenizer.index_word.get(predicted_id, '')\n",
    "\n",
    "        #if word == 'end' or word == '':\n",
    "         #   break\n",
    "\n",
    "        result.append(word)\n",
    "        target_seq = np.array([[predicted_id]])\n",
    "\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dece336b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Headline Predictions:\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_headline() missing 1 required positional argument: 'y_maxlen'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHeadline Predictions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m para \u001b[38;5;129;01min\u001b[39;00m test_paragraphs:\n\u001b[1;32m---> 11\u001b[0m     lstm_headline \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_headline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpara\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_model_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_model_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_maxlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_lstm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     gru_headline \u001b[38;5;241m=\u001b[39m generate_headline(para, encoder_model_gru, decoder_model_gru, tokenizer, x_maxlen, is_lstm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput Text      : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpara\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_headline() missing 1 required positional argument: 'y_maxlen'"
     ]
    }
   ],
   "source": [
    "\n",
    "test_paragraphs = [\n",
    "    \"The president addressed the media regarding the economic reforms.\",\n",
    "    \"India launched a new satellite into orbit for communication services.\",\n",
    "    \"The finance minister presented the annual budget in parliament.\",\n",
    "    \"Heavy rains caused flooding in several districts across the state.\",\n",
    "    \"The cricket team celebrated after winning the international tournament.\"\n",
    "]\n",
    "\n",
    "print(\"\\nHeadline Predictions:\\n\")\n",
    "for para in test_paragraphs:\n",
    "    lstm_headline = generate_headline(para, encoder_model_lstm, decoder_model_lstm, tokenizer, x_maxlen, is_lstm=True)\n",
    "    gru_headline = generate_headline(para, encoder_model_gru, decoder_model_gru, tokenizer, x_maxlen, is_lstm=False)\n",
    "    print(f\"Input Text      : {para}\")\n",
    "    print(f\"LSTM Headline   : {lstm_headline}\")\n",
    "    print(f\"GRU Headline    : {gru_headline}\")\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
